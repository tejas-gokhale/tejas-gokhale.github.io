<!DOCTYPE HTML>
<html lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Summer Vision Reading Group</title>
  
  	<meta name="author" content="Tejas Gokhale">
  	<meta name="viewport" content="width=device-width, initial-scale=1">
  
  	<link rel="stylesheet" type="text/css" href="stylesheet.css">
  	<link rel="icon" type="image/png" href="">
  	<style>
  		table, th, td {border: 1px solid black;}
  	</style>
</head>

<body>
	<h1> Summer Vision Reading Group </h1>
	<b> Who:</b> We're a bunch of PhD students across the CS, CE, and EE programs at ASU, UF, and Rice.
	<br>
	<b> What:</b>
	We read concepts in machine learning, and brainstorm their applications in frontier problems in computer vision, but not limited to vision.
	<br>
	<b> When, Where, How: </b>
	Send <a href="mailto:tgokhale@asu.edu">me</a> or <a href="mailto:matomot64@gmail.com">Joshua Feinglass</a> an email</a> if you want to join us! 
	<a href="https://asu.zoom.us/j/82554204804?pwd=bGJURnRvY1NRWm5RR0lGNUd4UDdJdz09">Zoom</a> Every Saturday 1300hrs MST
	<br>
	<br>
	<i>  </i>

	<table>
		<tr>
			<td> Date </td>
			<td> Host </td>
			<td> Topic </td>
			<td> Paper(s) Read</td>
			<td> Participants </td>
			<td> Notes </td>
		</tr>
		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > May 8, 2021 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Tejas Gokhale </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Uncertainty Sets for Image Classification </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Uncertainty Sets for Image Classifiers using Conformal Prediction, Angelopouls et al. ICLR 2021, <a href="https://openreview.net/pdf?id=eNdiU_DbM9">pdf</a> </li>
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Sheng Cheng, Joshua Feinglass, Tejas Gokhale, Blake Harrison, Ishan Khurjekar, Yiran Luo </td>
			<td style="padding:10px;width:20%;vertical-align:top"><i> uncertainty, prediction sets vs single prediction, coverage</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > May 15, 2021 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Blake Harrison </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Imagination in Navigation </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<!-- <li>Uncertainty Sets for Image Classifiers using Conformal Prediction, Angelopouls et al. ICLR 2021, <a href="https://openreview.net/pdf?id=eNdiU_DbM9">pdf</a> </li> -->
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Sheng Cheng, Joshua Feinglass, Tejas Gokhale, Blake Harrison, Yiran Luo</td>
			<td style="padding:10px;width:20%;vertical-align:top"><i> wave function collapse, exploration, imagination, voxels and graphs</i></td>
		</tr>

	</table>




<p>2020 Archive</p>
	<table>
		<tr>
			<td> Date </td>
			<td> Host </td>
			<td> Topic </td>
			<td> Paper(s) Read</td>
			<td> Participants </td>
			<td> Notes </td>
		</tr>
		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > May 16, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Tejas Gokhale </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Curriculum Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Curriculum Learning, Bengio et al. ICML 2009, <a href="https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf">pdf</a> </li>
					<li>On The Power of Curriculum Learning in Training Deep Networks, Hacohen & Weinshall, ICML 2019, <a href="https://arxiv.org/pdf/1904.03626.pdf#cite.bengio2009curriculum">pdf</a></li>
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Tejas Gokhale, Joshua Feinglass, Kowshik Thopalli, Man Luo, Zhiyuan Fang   </td>
			<td style="padding:10px;width:20%;vertical-align:top"><i> Connection with active learning, curriculum learning in GANs?, curriculum learning for domain adaptation</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > May 23, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Kowshik Thopalli / Tejas Gokhale </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Active Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Two Faces of Active Learning, Dasgupta, ALT 2009<a href="https://cseweb.ucsd.edu/~dasgupta/papers/twoface.pdf"> pdf</a> </li>
					<li>Active Learning for Deep Object Detection, Brust et al. 2018<a href="https://arxiv.org/abs/1809.09875"> pdf</a> </li>
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Kowshik Thopalli, Bhargav Ghanekar, Ishan Khurjekar, Joshua Feinglass, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i>Strategies for selecting samples to label, how to select the best samples that improve performance vs heuristic-based selection?, Schrodinger's Douchebags</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > May 30, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Kowshik Thopalli </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Multi-modal Fusion </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					
					<li>Multimodal Machine Learning:A Survey and Taxonomy, Baltru≈°aitis et al., TPAMI 2019 <a href="https://arxiv.org/pdf/1705.09406.pdf"> pdf</a> </li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Kowshik Thopalli, Bhargav Ghanekar, Ishan Khurjekar, Joshua Feinglass, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" >  <i> Use-cases, when is it critical, audio-video alignment, modalities at different sampling rates, self-driving,</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > June 6, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Ishan Khurjekar </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Uncertainty Estimation I </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles, Lakshminarayanan et al. NeurIPS 2017<a href="http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles"> pdf</a> </li>
										
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >  Ishan Khurjekar, Bhargav Ghanekar, Joshua Feinglass, Kowshik Thopalli, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" >
				<i>
					Uncertainty, epistemic vs sensing, uncerrtainty estimation --> stronger metrics for evaluation of models, 
				</i>
			</td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > June 13, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Joshua Feinglass </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Transformers, BERT, VilBERT/LXMERT</td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Attention is All You Need, Vaswani et al. NeurIPS 2017 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"> pdf</a>, <a href="http://jalammar.github.io/illustrated-transformer/"> blog</a></li>
					<li>BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding, Devlin et al. NAACL 2019 <a href="https://arxiv.org/pdf/1810.04805.pdf"> pdf</a> </li>
					<li>LXMERT, Tan et al. EMNLP 2019<a href="https://www.aclweb.org/anthology/D19-1514.pdf"> pdf</a> </li>
					<li>VilBERT, Lu et al, NeurIPS 2019<a href="https://arxiv.org/pdf/1908.02265.pdf"> pdf</a> </li>

					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Joshua Feinglass, Bhargav Ghanekar, Ishan Khurjekar, Kowshik Thopalli, Man Luo, Sheng Cheng, Tejas Gokhale</td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Attention, Self-Attention, Encoder-Decoder Architecture, Transformer Decoder, BERT pre-training tasks and intuition behind their choice, extension to cross-modal (vision+language) pre-training</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > June 20, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Bhargav Ghanekar </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Computational Imaging </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>PhaseCam3D, Wu et al. ICCP 2019<a href="https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/3/6740/files/2019/05/PhaseCam3D.pdf"> pdf</a> </li>

					<li>All-optical ML using diffractive deep NN, Lin et al. Science Mag 2018<a href="https://science.sciencemag.org/content/sci/361/6406/1004.full.pdf"> pdf</a> </li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Bhargav Ghanekar, Joshua Feinglass, Ishan Khurjekar, Kowshik Thopalli, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Point Spread Function, Relation between Defocus Blur and Depth, Designing Phase Masks through end-to-end learning</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > June 27, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Ishan Khurjekar </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Uncertainty Estimation II </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?, Kendall & Gall, NeurIPS 2017 <a href="https://arxiv.org/pdf/1703.04977.pdf"> pdf</a>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Ishan Khurjekar, Bhargav Ghanekar, Changhoon Kim, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i>  Combining Aleatoric and Epistemic Uncertainty. Heteroscedastic Uncertainty as Learned Loss Attenuation: (1) inputs with high predicted uncertainty will have a smaller effect on the loss, (2) model is discouraged from predicting very low uncertainty for points with high residual error,  </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > July 4, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Changhoon Kim</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Adversarial Attack </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Disrupting Image-Translation-Based DeepFake Algorithms with Adversarial Attacks, Yeh et al. WACV 2020<a href="https://openaccess.thecvf.com/content_WACVW_2020/papers/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.pdf"> pdf</a> </li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Changhoon Kim, Ishan Khurjekar, Joshua Feinglass, Kowshik Thopalli, Man Luo, Sheng Cheng, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Adversarial Attacks against malicious generative neural networks (DeepFake / DeepNude), extensions, generalization to unseen networks, ensembles</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > July 11, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Sheng Cheng</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Contrastive Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Momentum Contrast for Unsupervised Visual Representation Learning, He et al. CVPR 2020<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf"> pdf</a> </li>
					<li>A Simple Framework for Contrastive Learning of Visual Representations, Chen et al. ICML 2020<a href="https://arxiv.org/pdf/2002.05709.pdf"> pdf</a></li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Sheng Cheng, Bhargav Ghanekar, Ishan Khurjekar, Joshua Feinglass, Man Luo, Tejas Gokhale </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Contrastive Learning as a self-supervised learning framework, connection with hashing, auto-encoders, language pretraining, word2vec. Open discussion on many faces of generalization. </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > July 18, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Man Luo</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Neuro-Symbolic Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, Serafini & Garcez. 2016<a href="https://arxiv.org/pdf/1606.04422.pdf"> pdf</a> </li>
					<li>Logic Tensor Networks for Semantic Image Interpretation, Donadello et al. IJCAI 2017<a href="https://www.ijcai.org/Proceedings/2017/0221.pdf"> pdf</a> </li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >Man Luo, Sheng Cheng, Bhargav Ghanekar, Ishan Khurjekar, Joshua Feinglass, Tejas Gokhale</td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> symbolic logic, super-quick intro to knowledge representation (propositional, FOL), demo of a reasoning problem in CLINGO, logic tensor networks, open-ended discussion about application in vision-and-language.</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > July 25, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Tejas Gokhale</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Reinforcement Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Tutorial: Deep Reinforcement Learning, David Silver. ICML 2016 <a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"> pdf</a> </li>
					<li>Playing Atari with Deep Reinforcement Learning, Mnih et al. NIPS 2013, DL Workshop  <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"></li>
					
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >  Tejas Gokhale, Sheng Cheng, Bhargav Ghanekar, Ishan Khurjekar, Joshua Feinglass, Man Luo</td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> RL vocabulary: state, action, reward, policy, discount factor. intuition behind experential replay and discounted reward, simpler example: navigation from (0, 0) to (10, 10). SARSA. Breakthrough in learning to play Atari games.</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > August 1, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Joshua Feinglass</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Information Theory </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>A  Gentle  Tutorial  on Information  Theory  and  Learning, Roni Rosenfeld, CMU 1999, (CMU 11761<a href="https://www.cs.cmu.edu/~roni/11761/Presentations/IT-tutorial.pdf"> pdf</a> </li>		
					<li>Information Theory, Cosma Shalizi, Complex Systems Summer School 2010 <a href=""> pdf</a> </li>	
					<li>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, Chen et al. NeurIPS 2016. <a href="https://arxiv.org/abs/1606.03657"> pdf</a></li>	

				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Joshua Feinglass, Sheng Cheng, Ishan Khurjekar,  Man Luo, Tejas Gokhale  </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Definitions of information, mututal information, entropy, Fischer Information, random thoughts about Huffmann coding and Wavelet compression, InfoGAN, conditional GAN, pitfalls and tricks-of-the-trade for GAN training, mode collapse</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > August 8, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > - </td>
			<td style="padding:10px;width:10%;vertical-align:top" > cancelled </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li><a href=""> pdf</a> </li>		
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >   </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > August 15, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Man Luo</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Information Retrieval </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li><a href="https://docs.google.com/presentation/d/1aFRKSM9FVbQJ1skBqv-N-7dfjn1wnQDFFQ87IHktcc8/edit?usp=sharing"> slides</a> </li>		
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Man Luo,  Sheng Cheng, Joshua Feinglass, Tejas Gokhale, Ishan Khurjekar, Yiran Luo  </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > August 22, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Tejas Gokhale</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Convex Optimization </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Intro to Convex Optimization for ML, John Duchi, Stanford CS294, <a href="https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf"> pdf</a> </li>	
					<li>Input Convex Neural Networks, Amos et al. ICML 2017, <a href="http://proceedings.mlr.press/v70/amos17b/amos17b.pdf"> pdf </a></li> 	
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >  
			Tejas Gokhale, Sheng Cheng, Joshua Feinglass, Yiran Luo, Kuntal Pal.</td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Definitions of convex sets, functions. The general optimization problem. The convex optimization problem. Why is convexity nice? Lgrangian duality, steepest descent, gradient descent, proximal methods (projected GD), Newton. <br> Input convex NN (convex inference)</i></td>
		</tr>


		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > August 29, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Ishan Khurjekar</td>
			<td style="padding:10px;width:10%;vertical-align:top" > Graph Neural Networks </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Representation Learning on Networks Tutorial, Jure Leskovec, WWW18<a href="http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part2-gnns.pdf"> pdf</a> </li>
					<li>The graph neural network model, Scarselli et al. IEEE-TNN2009<a href="https://persagen.com/files/misc/scarselli2009graph.pdf"> pdf</a> </li>
					<li>Semi-Supervised Classification with Graph Convolutional Networks, Kipf & Welling, ICLR 2017<a href="https://arxiv.org/pdf/1609.02907.pdf"> pdf</a> </li>

				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > Ishan Khurjekar, Man Luo,  Yiran Luo, Sheng Cheng, Joshua Feinglass, Tejas Gokhale, Amrita Bhattacharjee, Weidong Zhang </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Graph neural networks, aggregation functions, training (supervises/unsupervised), some applications, aggregation as convolution.</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > September 5, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > - </td>
			<td style="padding:10px;width:10%;vertical-align:top" > cancelled </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li><a href=""> pdf</a> </li>		
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >   </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > September 12, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Tejas Gokhale </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Causal Inference </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Causal Inference: A Tutorial, Fan Li, Duke University<a href="http://www2.stat.duke.edu/~fl35/teaching/440-19F/Tutorial_PlusDS.pdf"> pdf</a> </li>
					<li>Amortized learning of neural causal representations, Ke et al. ICLR2020 CL Workshop<a href="https://arxiv.org/pdf/2008.09301.pdf"> pdf</a> </li>		
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > TG,BG,IK,JF,YL,SC,ML </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Causality notations: variables, interventions/treatments, outcomes, confounders, CI as a missing data problem, <br> Learning causal models with NN, results on synthetic data, missing pieces/restrictive assumptions for real-world data</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > September 19, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Joshua Feinglass (transfer hosting)/ Kowshik Thopalli (supervision) / Tejas Gokhale (meta-hosting) </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Meta Learning </td>
			<td style="width:30%;vertical-align:top" > 
				<ul>
					<li>Meta Learning Tutorial, Finn&Levine, ICML 2019<a href="https://sites.google.com/view/icml19metalearning"> link</a> </li>
					<li>MAML, Finn et al. ICML2017 <a href="https://arxiv.org/abs/1703.03400"> pdf</a> </li>		
				</ul>
			</td>
			<td style="padding:10px;width:20%;vertical-align:top" > JF,KT,TG,BG,IK,YL,SC,ML,AB </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> Learning, learning to learn, meta-train, meta-test, MAML algo discussion, use-cases, extensions with "unequal", "hierarchical", "unrelated" tasks, insights from KT about faster algos, taskonomy/task2vec ...</i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > September 26, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Yiran Luo </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Low Resource Machine Translation </td>
			<td style="width:30%;vertical-align:top" > 

			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >  </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> </i></td>
		</tr>

		<tr>
			<td style="padding:10px;width:10%;vertical-align:top" > October 03, 2020 </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Sheng Cheng </td>
			<td style="padding:10px;width:10%;vertical-align:top" > Super-resolution </td>
			<td style="width:30%;vertical-align:top" > 

			</td>
			<td style="padding:10px;width:20%;vertical-align:top" >  </td>
			<td style="padding:10px;width:20%;vertical-align:top" > <i> </i></td>
		</tr>




	</table>

	<h4>Very Good Machine Learning Classes </h4>
	<ul>
		<li><a href="http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml"> CMU 10-601, Intro. to ML (Nina Balcan) </a></li>
		<li><a href="https://work.caltech.edu/telecourse.html">Caltech CS156, Learning from Data (Yaser Abu-Mostafa)</a></li>
		<li><a href="http://www.cs.cmu.edu/%7E10715-f18/lectures.shtml">CMU 10-715, Adv. Intro. to ML (Nina Balcan / Tom Mitchell) </a></li>
		<li><a href="http://www.stat.cmu.edu/~ryantibs/statml/">CMU 36-702, Stat. ML (Tibshirani/Wasserman) </a></li>
		<li><a href="https://www.cs.cmu.edu/~epxing/Class/10708-20/lectures.html">CMU 10-708, Prob. Graphical Models (Eric Xing)</a></li>
	</ul>

	<br><br>
	<b> History: </b>
	This reading group was started by TG as a weekly reading group / social meetup for PhD students stranded and locked up inside their own houses due to the COVID19 pandemic.
	We decided to continue it after summer ended (does it ever end in AZ?) and were too lazy to change the name.
	S01 of the reading group started on May 16, 2020 and ended on Oct 03, 2020. 
	S02 resumed in May 2021, with Joshua Feinglass taking over as host.
	<br>
	<a href="./images/reading_group_20210529.jpg">First In-Person Meeting</a>

	<br><br><br><br><br><br>
	<hr>
	<p style="text-align:center;"><a href="./index.html">Tejas Gokhale</a></p>
</body>
</html>