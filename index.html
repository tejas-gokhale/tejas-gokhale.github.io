<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tejas Gokhale</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="theme.css">
    <link rel="stylesheet" type="text/css" href="font-awesome-4.7.0/css/font-awesome.css">
    <link href="academicons-1.9.3/css/academicons.css" rel="stylesheet" />
    <link rel="shortcut icon" type="image/png" href="images/tg_hawaii_square.png"/>
    
  </head>
<body>
  
<div class="container" style="background-color: #fcfcfc;">
  <div class="row mb-3 justify-content-center">
    <div class="col-md-4" style="font-family: Lato, Helvetica, arial, sans-serif;">
      <center><h1>Tejas <b>Gokhale</b></h1></center>
    </div>
    <div class="col-md-8">
      <div class="topnav">
        <!-- <br/><b>Tejas Gokhale</b> -->
        <a href="./faq.html">FAQ</a>
        <a href="#teaching">TEACHING</a>
        <a href="">PEOPLE</a>
        <a href="#publications">RESEARCH</a>
        <a href="./index.html">HOME</a>
      </div>
    </div>


    <div class="col-md-4">
      <center>
        <br/><img src="images/tg_hawaii_square.png" alt="Tejas Gokhale"/ style="max-width: 75%;">
        <p>
          <br/>Incoming Assistant Professor
          <br/>Computer Science
          <br/>University of Maryland, Baltimore County
        </p>
        <p> 
          <a href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ"><i class="ai ai-google-scholar-square ai-2x"></i></a>
          <a href="tgokhale_resume.pdf"><i class="ai ai-cv-square ai-2x"></i></a>
          <a href="https://www.linkedin.com/in/tejas-gokhale-78488a1a8/"><i class="fa fa-linkedin-square fa-2x"></i></a>
        </p>
        
        <p>
          <h5>Quick Links</h5>
          <a href="https://www.csee.umbc.edu/"> UMBC CSEE</a>
          <br/><a href="https://meyerhoff.umbc.edu/">Meyerhoff Scholars</a>
          <br/><a href="https://cwit.umbc.edu/cwitscholars/"> Center for Women in Technology</a>
          <br/><a href="https://ur.umbc.edu/"> Undergraduate Research Opportunities</a>
          <br/><a href="https://umbc.edu/about/"> UMBC Stats & Rankings </a>
        </p>
        <p>
          <h5>Teaching</h5>
        </p>
        <p id="officehours">
          <h5>Office Hours</h5>
        </p>
        <p>
          <a href="./bio.html">Third-Person Bio</a>
      </center>


    </div>

    <div class="col-md-8" style="padding-right:32px;">
      <br/> I am a computer vision researcher working towards the design of robust and reliable systems that can understand the visual world.
      My research draws inspiration from principles of perception, communication, learning, and reasoning.
      The two focii of my lab are to address reliability from the perspective of machine learning and from the perspective of use-inspired and human-centered computing.
      <br/>
      <br/> I received my Ph.D. from Arizona State University where I was advised by <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> and <a href="http://www.public.asu.edu/~cbaral/">Chitta Baral</a>, M.S. from Carnegie Mellon University where I worked with <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>, and B.E. from Birla Institute of Technology and Science. During my graduate studies I worked with wonderful collaborators at Lawrence Livermore National Laboratory, Microsoft Research, and Snap Research.
      <br/>
      <br/>
      <div>
        <div class="col-md-12" style="border-radius: 20px; background-color: #f5e1e1;padding-top:16px;padding-bottom:16px;padding-left:32px;padding-right:32px;">  
          <p style="font-weight:normal; margin:0px; color: black">
            <i class="fa fa-bullhorn"></i> 
            <b>Join the Group!</b>
            I am recruiting <b><a style="color: #ba5353" href="https://forms.gle/M9RupRRry8ftCX3d7">BS/MS/PhD researchers</a></b> to join me at UMBC in Fall 2023. 
            Please use this <b><a style="color: #ba5353" href="https://forms.gle/M9RupRRry8ftCX3d7">form</a></b> to apply!
            See <b><a style="color: #ba5353" href="https://forms.gle/M9RupRRry8ftCX3d7">FAQ</a></b> for more.
          </p>
        </div>
        <br/>
        <div class="col-md-12">
         <!-- style="border-radius: 20px; background-color: #f5f5d2;padding-top:16px;padding-bottom:16px;padding-left:32px;padding-right:32px;">  -->
          <p style="font-weight:normal; margin:0px; color: black">
            <center><h5> News </h5></center>
            <small><table class="color-table">
              <colgroup>
                <col style="width:30%">
                <col style="width:70%">
              </colgroup>
              <tbody>
                <tr>
                  <td>Jun 2023</td>
                  <td>Organizing <a href="https://asu-apg.github.io/odrum/" >O-DRUM 2023</a> (Workshop on Open-Domain Reasoning Under Multi-Modal Settings) at CVPR </td>
                </tr>
                <tr>
                  <td>Apr 2023</td>
                  <td>Defended my Ph.D !!!</td>
                </tr>
                <tr>
                  <td>Feb-Apr 2023</td>
                  <td>Invited Talks on "Reliable Semantic Vision" at 
                    <i>
                      <ul>
                        <li>Rochester Institute of Technology</li>
                        <li>SUNY Binghamton</li>
                        <li>Indiana University</li>
                        <li>University of Maryland Baltimore County</li>
                        <li>Case Western Reserve University</li>
                        <li>Colorado School of Mines</li>
                        <li>Temple University</li>
                      </ul>
                    </i>
                  </td>
                </tr>
                <tr>
                  <td>Jan 2023</td>
                  <td>Delivered <a href="https://asu-apg.github.io/serum/">SERUM (Tutorial on Semantic Data Engineering under Multimodal Settings) at WACV 2023</a>
                <tr>
                  <td>Oct 2022</td>
                  <td>Recognized as <a href="https://neurips.cc/Conferences/2022/ProgramCommittee">Top Reviewer for NeurIPS 2022 </a></td>
                </tr>
                <tr>
                  <td>Jun 2022</td>
                  <td>Organized <a href="https://asu-apg.github.io/odrum/archive_2022.html" >O-DRUM 2022</a> (1st Workshop on Open-Domain Retrieval Under Multi-Modal Settings) at CVPR </td>
                </tr>
              </tbody>
            </table></small>
          </p>
        </div>
      </div>
    </div>
  </div>


  <hr />

  <div class="row col-md-12 justify-content-left">
    <div class="col-md-12 mb-3" style="font-size:14px;">
      <a id="publications"><h3>Publications</h3></a>
      <small>* indicates equal contribution</small><br /><br />

      <!--       <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src=""></a>
        </div>
        <div class="col-md-8 mb-3">
          
        </div>
      </div> -->

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src="images/reviz_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="" class="papertitle">End-to-end Knowledge Retrieval with Multi-modal Queries</a>
          <br/><venue>ACL 2023 (preprint coming soon!)</venue>
          <br>
          <a href="https://luomancs.github.io/">Man Luo</a>, 
          <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang</a>,
          <b>Tejas Gokhale</b>, 
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br/>
          <!-- <a href="" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a> -->
          <br/>
          <p class="tag">
            Knowledge retrieval with multi-modal queries, i.e., queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval.
            A new dataset called ReMuQ, a new pretraining task for learning knowledge retrieval with multimodal queries, and a retriever model "ReViz" that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators.
          </p>
          
        </div>
      </div> 


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="https://arxiv.org/abs/2303.17080" class="teaser"><img src="images/mole_recruitment.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2303.17080" class="papertitle">Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling</a>
          <br/><venue>preprint</venue>
          <br>
          Ethan Wisdom, 
          <b>Tejas Gokhale</b>, 
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br/>
          <a href="https://arxiv.org/pdf/2303.17080.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <br/>
          <p class="tag">
            A data poisoning attack that confounds ML models without any manipulation of the image or label, achieved by simply leveraging the most confounding natural samples found within the training data itself.
            We show the efficacy of this novel attack in offline as well as continual learning (CL) settings in image classification, thereby exposing a previously undetected vulnerability of image classifiers.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="https://arxiv.org/abs/2212.10015" class="teaser"><img src="images/visor_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2212.10015" class="papertitle">Benchmarking Spatial Relationships in Text-to-Image Generation</a>
          <br/><venue>preprint</venue>
          <br>
          <b>Tejas Gokhale</b>, 
          <a href="https://hamidpalangi.com">Hamid Palangi</a>,
          <a href="https://besmiranushi.com">Besmira Nushi</a>,
          <a href="https://vibhav-vineet.github.io/">Vibhav Vineet</a>,
          <a href="https://erichorvitz.com/">Eric Horvitz</a>,
          <a href="https://ecekamar.com">Ece Kamar</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2212.10015.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://visort2i.github.io/" class="badge badge-code badge-sm text-decoration-none mb-1">web</a>

          <p class="tag">
             We report a surprising finding that, although recent state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations such as left/right/above/below.
             We introduce a metric called VISOR to quantify spatial reasoning performance. VISOR can be used off-the-shelf with any text-to-image model.
             We construct and make available SR2D, a dataset which contains sentences that describe spatial relationships (left/right/above/below) between a pair of commonly occurring objects.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src="images/alt_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2206.07736" class="papertitle">Improving Diversity with Adversarially Learned Transformations for Domain Generalization</a>
          <br/><venue> WACV 2023</venue>
          <br>
          <b>Tejas Gokhale</b>,
          <a href="https://rushila.com/">Rushil Anirudh</a>,
          <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
          <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2206.07736.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/ALT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://www.youtube.com/watch?v=3EktDD9JDNE" class="badge bg-white badge-sm text-decoration-none">video</a>

          <p class="tag">
            ALT discovers diverse and adversarial transformations using an image-to-image neural network with learnable weights.
            ALT improves the state-of-the-art single domain generalization performance on three benchmarks and is significantly better than pixel-wise adversarial training and standard data augmentation techniques.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/cripp_vqa_example.PNG" class="teaser"><img src="images/cripp_vqa_example.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2211.03779" class="papertitle">CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</a>
          <br/><venue> EMNLP 2022</venue>
          <br>
          Maitreya Patel, 
          <b>Tejas Gokhale</b>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2211.03779.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://maitreyapatel.com/CRIPP-VQA/" class="badge badge-code badge-sm text-decoration-none mb-1">web</a>

          <p class="tag">
            Although the imaging pipeline is unable to capture many physical properties of objects (eg. mass and coefficient of friction), these properties can be estimated by utilizing cues introduced by collisions. 
            We introduce a new dataset (CRIPP-VQA) for reasoning about the implicit physical properties of objects from videos.
            The dataset contains videos of objects in motion, annotated with hypothetical/counterfactual questions about the effect of actions (removing/adding/replacing objects) and questions about planning (performing actions to reach a goal).            
          </p>
        </div>
      </div>


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <center><a href="images/interpolate_teaser.png" class="teaser"><img src="images/interpolate_teaser.png"></a></center>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://openreview.net/pdf?id=YkPjTHZDdm" class="papertitle">Covariate Shift Detection via Domain Interpolation Sensitivity</a>
          <br/><font color="red"><b>[SPOTLIGHT!]</b></font> <venue>NeurIPS 2022 Workshop on Interpolation and Beyond</venue>
          <br>
          <b>Tejas Gokhale</b>,
          Joshua Feinglass, 
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://openreview.net/pdf?id=YkPjTHZDdm" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>

          <p class="tag">
            In this paper, we introduce a benchmark for covariate shift detection (CSD), that builds upon and complements previous work on domain generalization.
            We find that existing novelty detection methods designed for OOD benchmarks perform worse than simple confidence-based methods on our CSD benchmark.
            We propose Domain Interpolation Sensitivity (DIS), based on the simple hypothesis that interpolation between the test input and randomly sampled inputs from the training domain, offers sufficient information to distinguish between the training domain and unseen domains under covariate shift.
          </p>
        </div>
      </div>


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src="images/example_nlvr_violin.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2110.07165" class="papertitle">Semantically Distributed Robust Optimization for Vision-and-Language Inference</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b>Tejas Gokhale</b>,
          Abhishek Chaudhary, 
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/pdf/2110.07165.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/ASU-APG/VLI_SDRO" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <p class="tag">
            SDRO: a distributed robust optimization method that operates with linguistic transformations of sentence inputs, SISP: a suit of semantics-inverting (SI) and semantics-preserving (SP) linguistic transformations, and an ensembling technique for vision-and-language inference.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/genvsrob_teaser.png" class="teaser"><img src="images/genvsrob_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2203.07653" class="papertitle"><i>Generalized but not Robust?</i> Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b>Tejas Gokhale</b>, 
          <a href="https://luomancs.github.io/">Man Luo</a>, 
          <a href="https://scholar.google.com/citations?user=-7LK2SwAAAAJ">Swaroop Mishra</a>,
          <a href="https://www.linkedin.com/in/bhavdeep-singh-sachdeva-767a3b7a">Bhavdeep Singh Sachdeva</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2203.07653.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).
            This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/waldo_teaser.PNG" class="teaser"><img src="images/waldo_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2203.16682" class="papertitle">To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo</a>
          <br/><venue>ACL 2022</venue>
          <br>
          <a href="">Yiran Luo</a>, 
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2203.16682.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/fpsluozi/tofindwaldo" class="badge badge-code badge-sm text-decoration-none mb-1">data</a>
          <p class="tag">
            We present a debiased dataset for the Person Centric Visual Grounding (PCVG) task. For instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image). The debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src="images/bionr_qg.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2201.07745" class="papertitle">Improving Biomedical Information Retrieval with Neural Retrievers</a>
          <br/><venue>AAAI 2022</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Man Luo</a>,
          <a href=""> Arindam Mitra </a>,
          <b>Tejas Gokhale</b>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2201.07745.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            We seek to improve information retrieval (IR) using neural retrievers (NR) in the biomedical domain, using a three-pronged approach. (1) a template-based question generation method, (2) two novel pre-training tasks that are closely aligned to the downstream task of information retrieval, (3) the ``Poly-DPR'' model which encodes each context into multiple context vectors. 
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" class="teaser"><img src="images/spatial_vqa.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2109.01934" class="papertitle">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering</a>
          <br/><venue>ICCV 2021</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2109.01934.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            VQA models trained with two additional objectives: object centroid estimation and relative position estimation, lead to improved performance on spatial reasoning questions (in GQA) in fully supervised and few shot settings as well as improved O.O.D. generalization.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/synth_data.png"class="teaser"><img src="images/synth_data.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2012.02356" class="papertitle">WeaQA: Weak Supervision via Captions for Visual Question Answering</a>
          <br/><venue>ACL 2021 Findings</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.02356.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            We show that models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions.  Our experiments suggest gains on benchmark with shifted priors (VQA-CP) over baselines which use full supervision from human-authored QA data.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/teaser_ext.png"class="teaser"><img src="images/teaser_ext.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
          <br/><venue class="arxiv">CVPR 2021 Workshop</venue>, "AI for Content Creation"
          <br>
          <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
          <b>Tejas Gokhale</b>,
          <a href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
          <a href="https://pavanturaga.com/">Pavan Turaga</a>,
          <a href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
          <br>
          <a href="https://arxiv.org/pdf/2004.08614.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">             
            Scene completion from sparse and incomplete label maps.
            `Halluci-Net' is a 2-stage method that 
            captures the object co-occurrence relationships,
            to produce dense label maps from incomplete labelmaps and object boundaries,
            for image synthesis.
          </p>
        </div>
      </div>


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/ttl_rc.png"class="teaser"><img src="images/ttl_rc.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2103.11263" >
            Self-Supervised Test-Time Learning for Reading Comprehension
          </a>
          <br/><venue>NAACL 2021</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2103.11263.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>

          <p class="tag">
            Unsupervised Reading Comprehension method that operates directly on a single test passage.
            Synthetic QA pairs are generated from the passage, and models are trained on these.
            When a new human-authored test question appears, models infer answers better than previous unsupervised methods.
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/agat_overall_new.png"class="teaser"><img src="images/agat_overall_new.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
            Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
          </a>
          <br/><venue>AAAI 2021</venue>
          <br>
          <b>Tejas Gokhale</b>,
          <a href="https://rushila.com/">Rushil Anirudh</a>,
          <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.01806.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/AGAT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>

          <p class="tag">
            An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
            Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations --- object-related shifts, geometric transformations, and common image corruptions.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/overall_final.png"class="teaser"><img src="images/overall_final.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/abs/2009.08566" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
            We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
            MUTANT establishes a new SOTA (+10%) on the VQA-CP challenge (for generalization under Changing Priors)
          </p>            
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/v2c_tgokhale.png"class="teaser"><img src="images/v2c_tgokhale.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/pdf/2003.05162.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/jacobswan1/Video2Commonsense" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://asu-active-perception-group.github.io/Video2Commonsense/" class="badge bg-white badge-sm text-decoration-none">web</a>
          <p class="tag">
            Actions in videos are inherently linked to latent social and commonsense aspects.
            We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
            <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
            Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
          </p>      
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/lol_tgokhale.png"class="teaser"><img src="images/lol_tgokhale.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
          </a>
          <br><venue>ECCV 2020</venue>
          <br>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
          <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          
          <a href="https://arxiv.org/pdf/2002.08325.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>, 
          <a href="./vqa_lol.html" class="badge bg-white badge-sm text-decoration-none">web</a>
          <a href="https://youtu.be/u6dEKvwla9M" class="badge bg-white badge-sm text-decoration-none">video</a>
          <p class="tag">
            VQA models struggle at negation, antonyms, conjunction, disjunction!
            We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/blocksworld_tgokhale.PNG"class="teaser"><img src="images/blocksworld_tgokhale.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="papertitle">Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs
          </a>
          <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
          <br><b>Tejas Gokhale</b>,
          <a href="">Shailaja Sampat</a>,
          <a href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <br>
          <a href="https://arxiv.org/pdf/1905.12042.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>, 
          <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">[CVPR-VMC Paper]</a>
          <a href="https://asu-active-perception-group.github.io/bird_dataset_web/" class="badge bg-white badge-sm text-decoration-none">web</a>
          <p class="tag">
            Given two images (source, target) with different object configurations, 
            what is the sequence of steps to re-arrange source to match target?
            For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
            and exhibits inductive generalization.
          </p>
        </div>
      </div>
  </div>
</div>



<hr />
<div class="container">
  <div class="row mb-3 justify-content-center">

  <div class="col-xs-12 col-md-12 mb-3">


<hr style="border-top-color: black; margin: 0.1em auto;">
<p align='right'>
  <i>
    Website theme inspirations: 
    <a href="https://www.alanesuhr.com/">Alane Suhr</a>, 
    <a href="http://stevemacn.github.io/">Stephen MacNeil</a>, 
    <a href="https://jonbarron.info/">Jon Barron</a>
  </i>
</p>





<p hidden><a href=spam.html /a><p/>

</body>
</html>
