<!doctype html>
<html>
<head>
  <meta charset="UTF-8" http-equiv="X-UA-Compatible" content="text/html">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Tejas Gokhale">
  <meta charset="UTF-8">
  <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>  
  <title>Tejas Gokhale</title>
  <link rel="stylesheet" type="text/css" href="css/simpleGridTemplate.css" media="screen and (min-width: 1000px)">
  <link rel='stylesheet' media='screen and (max-width: 1000px)' href='css/simpleGridTemplate_small.css' />
  <link rel="icon" type="image/png" href="images/tgokhale.png">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">


  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">

  <meta name="mobile-web-app-capable" content="yes">

  <meta property="og:locale" content="en_US">
  <meta property="og:url" content="https://tejas-gokhale.github.io/">
  <meta property="og:site_name" content="papers">
  <link rel="canonical" href="https://tejas-gokhale.github.io/">
  <link rel="stylesheet" href="./assets/css/style.css">
  <link rel="icon" href="./favicon.png" type="image/png" />
  <link rel="manifest" href="./manifest.json">
  <link rel="apple-touch-icon" sizes="180x180" href="favicon-180.png">
  <link type="application/opensearchdescription+xml" rel="search" href="./search.xml"/>
  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      color: #808080;
      font-weight: 400;
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
  </style>
</head>
<body>
  <div class="gallery">
    <div class="thumbnail">
      
      <h4><font size="6">Tejas Gokhale</font></h4>
      <small>
        <table>
          <tr>
            <td style="width: 70%;height: 100px;padding-bottom:5px;">
              <p class="tag">
                <a href="https://cidse.engineering.asu.edu/">School of Computing (etc.)</a><br>
                Arizona State University
              </p>
              <!-- Contact -->
              <!-- <p class="tag">
                699 S Mill Ave. aka <a href="https://goo.gl/maps/A7JFuNGh32Pbmpv18">Brickyard Engineering</a>
              </p> -->
              <p class="tag" style="font-family:monospace"> tgokhale at asu dot edu </p>
              <!-- Google Scholar / Bio -->
              
                
              <p class="tag">
                <a href="https://www.public.asu.edu/~tgokhale/papers/"> Papers</a><br>
                <a href="https://www.semanticscholar.org/author/Tejas-Gokhale/120838645">Semantic Scholar</a> <br>
                <a href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ">Google Scholar</a>
                <br>
                <a href="https://asu-active-perception-group.github.io/seminar/">V&L Seminar Series</a><br>
                <a href="./reading_group.html">Reading Group</a>

                <!-- Website: <a href="https://tejas-gokhale.github.io/">GitHub</a> = <a href="https://www.public.asu.edu/~tgokhale/">ASU</a>  -->
              </p>
            </td>
            <td style="width: 30%;padding-right:0px;padding-bottom:5px;">
              <a href="#">
                <img src="images/tg_boston.jpg" width="160px" align="right"/>
              </a>
            </td>
          </tr>
        </table>
      </small>

      <div class="boxed">
        <p class="tag" style="text-align:justify;">
          <small>
            My first name <b>Tejas </b><a href="https://www.youtube.com/watch?v=5fFDayUQpb0"><i class='fas fa-volume-up'></i></a> rhymes with Latin names such as <i>Albus, Marcus, Columbus, etc.</i>
            My family name <b>Gokhale </b><a href="https://www.youtube.com/watch?v=lRorIh2RaSQ&t=8s"><i class='fas fa-volume-up'></i></a> is approximately <i>"Go Clay"</i>.
          </small>
        </p>
      </div>
      <hr style="height:0pt; visibility:hidden;" />

      <h4><font size="4">BIO</font></h4>
      <small>
        <p class="tag" style="text-align:justify;">
          I am a PhD student at Arizona State University</a>, where I work with <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> and <a href="http://www.public.asu.edu/~cbaral/">Chitta Baral</a>. I've worked as a research intern at <a href="https://computing.llnl.gov/casc/ml">Lawrence Livermore National Labs</a> (Summer 2021, Summer 2020) and <a href="https://research.snap.com/team/category/computational-imaging/">Snap Inc.</a> (Summer 2018).
          I received my Masters from <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a> where I worked with <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>. 
          <br>
          <!-- Even before that, I did my Bachelors at <a href="https://www.bits-pilani.ac.in/">BITS Pilani</a> where I did a bunch of different things. -->
        </p> 
      </small> 
      <hr style="height:0pt; visibility:hidden;" /> 


      <!-- Research Interest -->
      <h4><font size="4">RESEARCH</font></h4>
      <small> 
        <p class="tag" style="text-align:justify;">
          My core interests are broadly in computer vision and machine learning.
          The main focus of my Ph.D. is on robust visual understanding.
          <!-- I work on computer vision, and in order to "understand" the visual world, work at the intersection of <strong>vision and language</strong>.  -->
          More specifically, I am interested in the following questions:
          <ul class="tag">
            <li> Can language help in improved understanding of vision?</li>
            <li> Do physical signals (such as depth, orientation, mass, material properties) impact our linguistic understanding of a scene?</li>
            <li> What mechanisms could allow models to have greater generalization guarantees?</li>
          </ul>
        </p>
      </small> 
      <hr style="height:0pt; visibility:hidden;" />


      <!-- Team -->
      <h4><font size="4">PEOPLE</font></h4>
      <h4><font size="2">Collaborators</font></h4>
      <small>
        <ul class="tag"> 
          <li> <strong>(ASU)</strong> &nbsp;
            <!-- <span style="float:right;"> -->
            <a href="http://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>, 
            <a href="https://pratyay-banerjee.github.io">Pratyay Banerjee</a>
          <!-- </span> -->
          </li>
          <li>
            <strong>(CMU/Adobe)</strong>&nbsp;
            <!-- <span style="float:right;">-->
            <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>
            <!-- </span> -->
          </li>
          <li>
            <strong>(LLNL)</strong>&nbsp;
            <!-- <span style="float:right;"> -->
            <a href="https://rushila.com/">Rushil Anirudh</a>, 
            <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
            <a href="https://jjthiagarajan.com/">Jay Thiagarajan</a>
            <!-- </span> -->
          </li>
        </ul>
      </small>

      <h4><font size="2">Mentees</font></h4>
      <small>
        <ul class="tag">
          <li><strong>(MS Research)</strong> 
            <br>Adela Shao (2021- )
            <br>Maitreya Patel (2021- )
            <br>Abhishek Chaudhary (2020-2021 AY) --> Amazon 
            <br>Arnav Chakravarthy (2020-2021 AY) --> VMWare
            <br><a href="https://www.linkedin.com/in/aadhavan-sadasivam/">Aadhavan Sadasivam</a> (2019-20 AY) --> PayPal
            <br>Shivakshit Patri (2019 Spring) --> Amazon
          </li>

          <li><strong><a href="https://furi.engineering.asu.edu/">(ASU FURI)</a></strong> <a href="https://www.linkedin.com/in/mertay-dayanc/">Mertay Dayanc</a></li>

          <li><strong>(BS Capstone)</strong>
            <a href="https://www.linkedin.com/in/paulfbutler2016">Paul Butler</a> (Video QA) --> Microsoft, 
            Jace Lord,
            Sagarika Pannase, 
            William Tith, 
            Aashwin Ranjan.
          </li>
        </ul>
      </small>

      <h4><font size="2">Friends who "Science"</font></h4>
      <small>
        <ul class="tag">
          <li><strong>(ASU)</strong>                
            <a href="https://kowshikthopalli.github.io/">Kowshik Thopalli</a>,          
            <a href="https://www.public.asu.edu/~lcheng35/">Lu Cheng</a>, 
            <!-- <a href="https://www.linkedin.com/in/aurghob/">Aurgho Bhattacharjee</a>,  -->
            <!-- <a href="https://aadhavansadasivam.com/">Aadhavan Sadasivam</a>,  -->
            <a href="http://samrawal.com/">Sam Rawal</a>, 
            <a href="https://www.linkedin.com/in/john-janiczek/">John Janiczek</a>, 
            <!-- <a href="https://andburch.github.io/">Andrew Burchill</a>,  -->
            <!-- <a href="https://scholar.google.com/citations?user=DXQIKGUAAAAJ">Tyler Quigley</a>,  -->
            <a href="https://www.taeyeongchoi.com/">Taeyeong Choi</a>, 
            <a href="https://www.linkedin.com/in/man-luo-a7aa57178/">Man Luo</a>
          </li>
          <li><strong>(CMU)</strong> 
            <a href="https://users.ece.cmu.edu/~vsaragad/">Vishwanath Saragadam</a>, 
            <!-- <a href="https://scholar.google.com/citations?user=LNXNkLEAAAAJ">Malhar Chaudhari</a>,  -->
            <a href="https://www.semanticscholar.org/author/Mihir-Hasabnis/103233313">Mihir Hasabnis</a>, 
            <a href="https://www.linkedin.com/in/bhargavghanekar">Bhargav Ghanekar </a>, 
            <a href="https://rachel-sunrui.github.io/"> Rachel Sun </a>
          </li>
          <li><strong>(BITS)</strong>
            <a href="https://scholar.google.com/citations?user=WyVypxAAAAAJ">Ninad Kanetkar (NEU BioEng)</a>, 
            <a href="https://www.leibniz-inm.de/en/staff/m-sc-bhusari-shardul-2/">Shardul Bhusari (Leibniz Institute Saarbrucken)</a>,
            <a href="http://ishankhurz.mystrikingly.com/">Ishan Khurjekar (UF ECE)</a>, 
            <a href="">Aseem Pradhan (GMU BioEng)</a>
          </li>
        </ul>
      </small>
      <hr style="height:0pt; visibility:hidden;" />

      <h4><font size="4">SERVICE</font></h4>
      <small>
        <ul class="tag">
          <li><strong>Reviewer:</strong> 
            <ul class="tag">
              <li><i>Learning/"AI":</i>
                <a href="https://iclr.cc/Conferences/2022"> ICLR 2022</a>, <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>, <a href="https://aaai.org/Conferences/AAAI-21/aaai21demoscall/">AAAI Demos 2022, 2021</a>
              </li>
              <li><i>NLP:</i>
                <a href="https://2021.emnlp.org/">EMNLP 2021</a>,
                <a href="https://2021.naacl.org/">NAACL 2021</a>,
              </li>
              <li><i>Robotics:</i>
                ICRA <a href="https://www.icra2019.org/">2019</a>, <a href="https://www.icra2020.org/">2020</a>, <a href="http://www.icra2021.org/">2021</a>, <a href="https://www.ieee-ras.org/publications/ra-l">IEEE RA-L</a>, 
              </li>
              <li><i>Vision:</i>
                <a href="https://www.springer.com/journal/138">Springer MVAP</a> 
              </li>
            </ul>
          </li>
          <li><strong>Organizer / Host:</strong>
            <br>&emsp;&emsp;<a href="https://asu-active-perception-group.github.io/seminar/">Spring 2021 Seminar Series (Frontiers in Vision and Language)</a>
            <br>&emsp;&emsp;<a href="reading_group.html">Summer Vision Reading Group</a>
          </li>
          <li><strong>Volunteer:</strong>
            <a href="https://icml.cc/Conferences/2020">ICML 2020</a>,
            <a href="https://swrobotics.engineering.asu.edu/wp-content/uploads/2019/10/SWRS_Program_2019_final.pdf">SWRS 2019</a>
          </li>
          <li><strong>Research Mentor:</strong> ASU FURI, CSE485 Capstone (Cognitive Vision, Vision&Language)</li>
          <li><strong>Teaching:</strong>
            <br>&emsp;&emsp;ASU CSE310: Data Structures and Algorithms (Taught Recitations) 
            <br>&emsp;&emsp;ASU CSE408: Multimedia Information Systems (TA)
            <br>&emsp;&emsp;ASU CSE110: Principles of Programming (Taught Labs)
            <br>&emsp;&emsp;ASU CSE576: Natural Language Processing (Mentored Class Projects)
            <br>&emsp;&emsp;BITS CTE: Advanced Image Processing (co-Instructor)
          </li>
          <li><strong>Student Mentor:</strong> 
            <br>&emsp;&emsp;Graduate Student Mentorship Program (GSMP), ASU, 2019-present.
            <br>&emsp;&emsp;Peer Mentorship Program (first installment), BITS Goa, 2014
          </li>
        </ul>          
      </small>

      <!-- 
      <h4>&nbsp;</h4> 

      <h4><font size="4">MISCELLANEA</font></h4>
      
      <p class="tag"><small>
        <b> Languages:</b> I am a native speaker of <a href="https://en.wikipedia.org/wiki/Marathi_language">Marathi</a> (~100M speakers), <a href="https://en.wikipedia.org/wiki/Hindustani_language">Hindustani*</a> (~1.2B speakers), and <a href="https://en.wikipedia.org/wiki/English_language">English</a>^ (~1.4B speakers)</a>.
        <br>*The question of the distinction between modern <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a> and modern <a href="https://en.wikipedia.org/wiki/Urdu">Urdu</a> is merely a political one.  Both are quintessentially Indic languages and are inseparable from one another in practice. (~1.2B speakers)<br>
        I can also understand basic <a href="https://en.wikipedia.org/wiki/Konkani_language">Konkani</a> (~3M speakers) -- I am progressively getting worse at it.
        I can also understand basic <a href="">Spanish</a> (~600M speakers) -- the more Juan Luis Guerra songs I listen to, the better I get at Spanish.
        <br>
          I can read and write fluently in <a href="https://en.wikipedia.org/wiki/Latin_script">Roman</a> (~2.5B users) and <a href="https://en.wikipedia.org/wiki/Devanagari">Devanagari</a> (~1B users) scripts.
          I can also read and (slowly) write in the <a href="https://en.wikipedia.org/wiki/Nastaliq">Arabic</a> script (~1B users).
      </p> -->
    </div> 


    <div class="thumbnail2">
      <!-- Affiliations -->
      <h4><font size="4">AFFILIATIONS</font></h4>
      <table align="center">
        <tr>
          <td align="center">
            <a href="https://www.llnl.gov/">
              <img src="images/llnl_logo.jpg" class="logo">
            </a>
          </td>
          <td align="center">
            
            <a href="http://www.asu.edu">
              <img src="images/asu_logo.png" class="logo">
            </a>
          </td>
          <td align="center">
            <a href="http://www.cmu.edu">
              <img src="images/cmu_logo.png" class="logo">
            </a>
          </td>
          <td align="center">
            <a href="https://research.snap.com/">
              <img src="images/snap_logo.png" class="logo">
            </a>
          </td>
          <td align="center">
            <a href="https://www.bits-pilani.ac.in/">
              <img src="images/bits_logo.png" class="logo">
            </a>
          </td>
          <td align="center">
            <a href="http://hkn.ece.cmu.edu/">
              <img src="images/hkn_logo.png">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center"><small><strong>Lawrence Livermore <br>National Laboratory</strong><br />Summer 2020, 2021</small></td>
          <td align="center"><small><strong>Arizona State <br>University</strong><br />2018-present</small></td>
          <td align="center"><small><strong>Carnegie Mellon <br>University</strong><br />2016-2018</small></td>
          <td align="center"><small><strong>Snap Inc. <br>Research</strong><br />Summer 2018</small></td>
          <td align="center"><small><strong>BITS <br>Pilani<br /></strong>2011-2015</small></td>
          <td align="center"><small><strong>IEEE <br>Eta Kappa Nu</strong><br />2017-present</small></td>
        </tr>
      </table>
      <hr style="height:0pt; visibility:hidden;" /> 
      <!-- Publication List -->
      <h4><font size="4">SELECTED PUBLICATIONS</font></h4>
      <small>
      <table style="border-collapse:separate; border-spacing:0 1em; width: 98%; margin-left: auto; margin-right: auto;">
        <tbody>
          <tr>
            <td class="teaser">
              <a href=""><img src="images/spatial_vqa.png" class="teaser"></a>
              <p>
              </p>
            </td>
            <td class="panel">
              <a href="" class="papertitle">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering</a>
              <br/><venue>ICCV 2021</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <strong>Tejas Gokhale</strong>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
              <p class="tag" style="text-align:justify">
                VQA models trained with two additional objectives: object centroid estimation and relative position estimation, lead to improved performance on spatial reasoning questions (in GQA) in fully supervised and few shot settings as well as improved O.O.D generalization.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/synth_data.png"><img src="images/synth_data.png" class="teaser"></a>
              <p>
                <a href="https://arxiv.org/pdf/2012.02356.pdf">Paper</a>
              </p>
            </td>

            <td class="panel">
              <a href="https://arxiv.org/abs/2012.02356" class="papertitle">WeaQA: Weak Supervision via Captions for Visual Question Answering</a>
              <br/><venue>ACL 2021 Findings</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <strong>Tejas Gokhale</strong>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>

              
              <p class="tag" style="text-align:justify">
                We show that models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions.  Our experiments suggest gains on benchmark with shifted priors (VQA-CP) over baselines which use full supervision from human-authored QA data.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/teaser_ext.png"><img src='images/teaser_ext.png' class="teaser"></a>
              <p>
                <a href="https://arxiv.org/pdf/2004.08614.pdf">Paper</a>
              </p>
            </td>
            <td class="panel">
              <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
              <br/><venue class="arxiv">CVPR 2021 Workshop</venue>, "AI for Content Creation"
              <br>
              <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
              <strong>Tejas Gokhale</strong>,
              <a href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
              <a href="https://pavanturaga.com/">Pavan Turaga</a>,
              <a href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
              <p class="tag" style="text-align:justify">             
                Scene completion from sparse and incomplete label maps.
                `Halluci-Net' is a 2-stage method that 
                captures the object co-occurrence relationships,
                to produce dense label maps from incomplete labelmaps and object boundaries,
                for image synthesis.
                <!-- HalluciNet outperforms single-stage baseline me thods on various performance metrics. -->
              </p>
              
            </td>
          </tr>
          <tr>
            <td class="teaser">
                <a href="images/ttl_rc.png"><img src="images/ttl_rc.png" class="teaser"></a>
                <p>
                  <a href="https://arxiv.org/pdf/2103.11263.pdf">Paper</a>
                </p>
            </td>

            <td class="panel">
              <a class="papertitle" href="https://arxiv.org/abs/2103.11263" >
                Self-Supervised Test-Time Learning for Reading Comprehension
              </a>
              <br/><venue>NAACL 2021</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <strong>Tejas Gokhale</strong>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>

              <p class="tag" style="text-align:justify">
                Unsupervised Reading Comprehension method that operates directly on a single test passage.
                Synthetic QA pairs are generated from the passage, and models are trained on these.
                When a new human-authored test question appears, models infer answers better than previous unsupervised methods.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
                <a href="images/agat_overall_new.png"><img src="images/agat_overall_new.png" class="teaser"></a>
                <p>
                  <a href="https://arxiv.org/pdf/2012.01806.pdf">Paper</a>
                </p>
            </td>

            <td class="panel">
              <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
                Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
              </a>
              <br/><venue>AAAI 2021</venue>
              <br>
              <strong>Tejas Gokhale</strong>,
              <a href="https://rushila.com/">Rushil Anirudh</a>,
              <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
              <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>

              <p class="tag" style="text-align:justify">
                An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
                Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations --- object-related shifts, geometric transformations, and common image corruptions.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/overall_final.png"><img src="images/overall_final.png" class="teaser"></a>
              <p>
                <a href="">Website (coming soon!)</a>, <a href="https://arxiv.org/abs/2009.08566">Paper</a>
              </p>
            </td>

            <td class="panel">
              <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
              </a>
              <br/><venue>EMNLP 2020</venue>
              <br>
              <strong>Tejas Gokhale*</strong>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              
              <p class="tag" style="text-align:justify">
                MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
                We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
                MUTANT establishes a new SOTA (+10%) on the VQA-CP challenge (for generalization under Changing Priors)
              </p>
              
              
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/v2c_tgokhale.png"><img src='images/v2c_tgokhale.png' class="teaser"></a>
              <p>
                <a href="https://asu-active-perception-group.github.io/Video2Commonsense/">Website</a>, <a href="https://arxiv.org/pdf/2003.05162.pdf">Paper</a>
              </p>
            </td>

            <td class="panel">
              <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
              </a>
              <br/><venue>EMNLP 2020</venue>
              <br>
              <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
              <strong>Tejas Gokhale*</strong>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              
              <p class="tag" style="text-align:justify">
                Actions in videos are inherently linked to latent social and commonsense aspects.
                We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
                <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
                Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
              </p>      
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/lol_tgokhale.png"><img src='images/lol_tgokhale.png' class="teaser"></a>
              <p>
                <a href="./vqa_lol.html">Website</a>, <a href="https://arxiv.org/pdf/2002.08325.pdf">Paper</a>, <a href="https://youtu.be/u6dEKvwla9M">Video</a>
              </p>
            </td>


            <td class="panel">
              <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
              </a>
              <br><venue>ECCV 2020</venue>
              <br>
              <strong>Tejas Gokhale*</strong>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
              <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              

              <p class="tag" style="text-align:justify">
                VQA models struggle at negation, antonyms, conjunction, disjunction!
                We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
              </p>
              
            </td>
          </tr>



          <tr>
            <td class="teaser">
              <a href="images/blocksworld_tgokhale.PNG"><img src='images/blocksworld_tgokhale.PNG' class="teaser"></a>
              <p><a href="https://asu-active-perception-group.github.io/bird_dataset_web/">Website</a>, <a href="https://arxiv.org/pdf/1905.12042.pdf">Longer Preprint</a>, <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf">CVPR-VMC Paper</a></p>

            </td>


            <td class="panel">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="papertitle">Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs
              </a>
              <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
              <br><strong>Tejas Gokhale</strong>,
              <a href="">Shailaja Sampat</a>,
              <a href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,

            
              <p class="tag" style="text-align:justify">
                Given two images (source, target) with different object configurations, 
                what is the sequence of steps to re-arrange source to match target?
                For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
                and exhibits inductive generalization.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
    </div> 

    <div class="thumbnail", style="max-width:300px;">
      <h4><font size="4">NEWS</font></h4>
      <p class="tag"><small>
        <strong><font color="000000">May 2021:</font></strong>
        Back as an intern at Lawrence Livermore National Laboratory, mentored by Rushil Anirudh, Jay Thiagarajan, and Bhavya Kailkhura
      </small></p>
      <p class="tag"><small>
        <strong><font color="000000">May 2021:</font></strong>
        Restarted the <a href="./reading_group.html">Summer Vision Reading Group</a>
      </small></p>
      <!-- <p class="tag"><small>
        <strong><font color="000000">Apr 2021:</font></strong>
        Halluci-Net will be presented at the <a href="http://visual.cs.brown.edu/workshops/aicc2021/">CVPR 2021 "AI for Content Creation" workshop</a>.
      </small></p> -->
      <!-- <p class="tag"><small>
        <strong><font color="000000">Mar 2021:</font></strong>
        A shorter version of AGAT will be presented at the <a href="https://sites.google.com/connect.hku.hk/robustml-2021/home">ICLR 2021 RobustML workshop</a>.
      </small></p> -->
      <p class="tag"><small>
        <strong><font color="000000">Mar 2021:</font></strong>
        I was awarded the CIDSE Doctoral Fellowship for Spring 2021.</a>.
      </small></p>
      <!-- <p class="tag"><small>
        <strong><font color="000000">Mar 2021:</font></strong>
        "Self-Supervised Test-Time Learning for Reading Comprehension" -- work pioneered by <a href="https://pratyay-banerjee.github.io/">Pratyay</a> accepted to NAACL 2021! <a href='https://arxiv.org/abs/2103.11263'>Preprint.</a>
      </small></p> -->
      <p class="tag"><small>
        <strong><font color="000000">Jan 2021:</font></strong>
        Co-organizing the <a href="https://asu-active-perception-group.github.io/seminar/">ASU-APG Seminar Series </a> on "Frontier Topics in Vision and/or Language" in Spring 2021.
      </small></p>
      <!-- <p class="tag"><small>
        <strong><font color="000000">Dec 2020:</font></strong>
        <a href="https://arxiv.org/abs/2012.01806">AGAT (Attribute Guided Adversarial Training)</a> was accepted to AAAI 2021. <a href="https://github.com/ASU-Active-Perception-Group/"><font color="maroon">Dataset</font></a>
      </small></p> -->
      <!-- <p class="tag"><small>
        <strong><font color="000000">Sep 2020:</font></strong>
        <a href="https://arxiv.org/abs/2009.08566">VQA-MUTANT</a> was accepted to EMNLP 2020. </a> <a href="https://github.com/ASU-Active-Perception-Group/vqa_mutant"><font color="maroon">Code</font></a>
      </small></p> -->

      <p class="tag"><small>
        <strong><font color="000000">Oct 2020:</font></strong>
        Hosted the first "Season" of <a href="./reading_group.html"> Summer Vision Reading Group </a> at ASU. (May-Oct 2020)
      </small></p>

      <!-- <p class="tag"><small>
        <strong><font color="000000">Sep 2020:</font></strong>
        <a href="https://arxiv.org/pdf/2003.05162.pdf">V2C (Video to Commonsense)</a> was accepted to EMNLP 2020 <a href="https://asu-active-perception-group.github.io/Video2Commonsense/">Website</a>, <a href="https://github.com/jacobswan1/Video_S3D_Talker"><font color="maroon">Code</font></a>
      </small></p> -->

      <!-- <p class="tag"><small>
        <strong><font color="000000">Jul 2020:</font></strong>
        <a href="https://arxiv.org/pdf/2002.08325.pdf">VQA-LOL: VQA under the Lens of Logic</a> accepted to ECCV 2020! <a href="https://youtu.be/u6dEKvwla9M"><font color="magenta">Talk</font></a>, <a href="./vqa_lol.html">Website</a>, <a href="https://github.com/ASU-Active-Perception-Group/vqa_lol"><font color="maroon">Code</font></a>
      </small></p> -->

      <p class="tag"><small>
        <strong><font color="000000">May 2020:</font></strong>
        Internship at Lawrence Livermore National Labs mentored by <a href="https://www.rushila.com">Rushil Anirudh</a>
      </small></p>

      <!-- <p class="tag"><small>
        <strong><font color="000000">Apr 2020:</font></strong>
        Halluci-Net (work done at CMU) <a href="https://arxiv.org/pdf/2004.08614.pdf">Preprint</a> is now available!
      </small></p> -->

      <p class="tag"><small>
        <strong><font color="000000">Mar 2020:</font></strong>
        I was awarded the CIDSE Doctoral Fellowship and the Engineering Graduate Fellowship.</a>.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Aug 2019:</font></strong>
        "Vision beyond Pixels", <i><a href='./presentations/tgokhale_ijcai2019dc.pdf'> <font color="magenta">Talk</font> </a></i> at IJCAI 2019 Doctoral Consortium, <i><font color="ForestGreen">Received IJCAI DC Travel Award</font></i>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jul 2019:</font></strong>
        "Reasoning about Objects and Actions via Block-Play", <font color="magenta">Talk</font> at Telluride 2019 <a href="https://sites.google.com/view/telluride2019/home">Neuromorphic Cognition Engineering Workshop</a>
      </small></p>

      <!-- <p class="tag"><small>
        <strong><font color="000000">Jun 2019:</font></strong>
        "Cooking with Blocks", Workshop paper presented at <a href="http://cvpr2019.thecvf.com/">CVPR</a> 2019, <font color="ForestGreen"><i>Received CIDSE Travel Grant for attending</i>
      </small></p> -->

      <p class="tag"><small>
        <strong><font color="000000">Aug 2018:</font></strong>
        Joined <a href="https://yezhouyang.engineering.asu.edu/research-group/">ASU Active Perception Group </a> as a PhD student with Yezhou Yang. Co-advised by Chitta Baral.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">May 2018:</font></strong>
        Internship at Snap Research, Seattle with <a href="https://www.linkedin.com/in/krishnanguru/">Guru Krishnan</a> and <a href="http://www.cs.columbia.edu/~nayar/">Shree Nayar</a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Apr 2018:</font></strong>
        "Deep Learning Methods in Imaging and Computer Vision", <i><a href="https://touch.facebook.com/AlumniRelationsBITSGoa/photos/a.577537568978007/1797680853630333/?type=3&source=54"> <font color="magenta">Invited Talk</font></a></i> at BITS Pilani.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Dec 2017:</font></strong>
        Graduated with M.S. (ECE) from <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a> (with <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu </a> Honors) 
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jan 2017:</font></strong>
        Joined <a href="http://imagesci.ece.cmu.edu/">Image Science Lab</a> at CMU as a Graduate Researcher with Aswin Sankaranarayanan.
      </small></p>

      <h4>&nbsp;</h4>


      <h4><font size="4">AWARDS and HONORS</font></h4>
      <h4><font size="2">Scholarships / Fellowships</font></h4>
      <ul class="tag">
        <li> <a href="http://www.ncert.nic.in/programmes/talent_exam/index_talent.html">National Talent Scholarship</a>, (Govt. of India, 2007-2015)</li>
        <li> Engineering Graduate Fellowship</a> (ASU Engineering, Spring 2020)</li>
        <li> <a href="https://graduate.asu.edu/current-students/funding-opportunities/awards-and-fellowships/university-graduate-fellowships"> CIDSE Doctoral Fellowship</a> (CIDSE, ASU, Spring 2020, Spring 2021)</li>
      </ul>

      <h4><font size="2">Travel Awards</font></h4>
      <ul class="tag">
        <li>Graduate College Travel Award, ASU (for EMNLP 2020)</li>
        <li>Graduate College Travel Award, ASU (for ECCV 2020)</li>
        <li> IJCAI Doctoral Consortium Travel Award, (IJCAI, 2019)</li>
        <li> CIDSE Travel Grant Award, (for CVPR 2019) </li>
      </ul>

      <h4><font size="2">Societies / Memberships</font></h4>
      <ul class="tag">
        <li> Inducted, <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu Sigma Chapter</a> (Carnegie Mellon University, 2017)</li>
        <li> Member, <a href="https://www.thecvf.com/">Computer Vision Foundation</a>, <a href="https://www.aclweb.org">Association for Computational Linguistics</a>, <a href="https://aaai.org/">Association for the Advancement of Artificial Intelligence</a></li> 
      </ul>           
    </div>
  </div>
</body>
</html>