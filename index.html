<!doctype html>
<html>
<head>
  <meta charset="UTF-8" http-equiv="X-UA-Compatible" content="text/html">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Tejas Gokhale">
  <meta charset="UTF-8">
  <title>Tejas Gokhale</title>
  <link href="css/simpleGridTemplate.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/tgokhale.png">
</head>
<body>
  <div class="gallery">
    <div class="thumbnail">
      <!-- Name & Photo -->
      <a href="#"><img src="images/tgokhale.png" width="140" align="right"/></a>
       <h4><font size="6">Tejas Gokhale</font></h4>
      <!-- Affliation -->
      <p class="tag">Ph.D. Student</p>
      <small>
      <p class="tag">
        <a href="https://cidse.engineering.asu.edu/">School of Computing (etc.)</a><br>
        Arizona State University
      </p>
      <!-- Contact -->
      <p class="tag">
        699 S Mill Ave. aka <a href="https://goo.gl/maps/A7JFuNGh32Pbmpv18">Brickyard Engineering</a>
      </p>
      <p class="tag"style="font-family:monospace"> tgokhale at asu dot edu </p>
      <!-- Google Scholar / Bio -->
      <p class="tag">
      <!-- Google Scholar / Bio -->
        <!-- <a href="short_bio.txt"><font size="4">Short Bio</font></a> &nbsp; &nbsp; -->
        <a href="https://www.semanticscholar.org/author/Tejas-Gokhale/120838645">Semantic Scholar</a> <br>
        Website: <a href="https://tejas-gokhale.github.io/">GitHub</a> = <a href="https://www.public.asu.edu/~tgokhale/">ASU</a> 
      </p>
      </small>
      <div class="boxed">
        
        <p class="tag"><small>
          <b style="color:crimson">NEW!</b>
          <i> Co-organizing the <a href="https://asu-active-perception-group.github.io/seminar/">ASU-APG Seminar Series </a> on "Frontier Topics in Vision and/or Language" in Spring 2021. </i>
          <br><br>

          <i> I started the <a href="./reading_group.html"> Summer Vision Reading Group </a> at ASU. (S01 ended on Oct 03, 2020 -- will resume in 2021)</i>
        </small></p>
      </div>
      <h4>&nbsp;</h4>

      <h4><font size="4">BIO</font></h4>
      <p class="tag"><small>
      I am a PhD student at Arizona State University</a>, where I work with <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> and <a href="http://www.public.asu.edu/~cbaral/">Chitta Baral</a>. 
      <br>
      In a previous life, I got my Masters from Carnegie Mellon University where I worked with <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>. 
      <br>
      Even before that, I did my Bachelors at BITS Pilani where I did a bunch of different things.
      </small></p>
      <h4>&nbsp;</h4>
      


      <!-- Research Interest -->
      <h4><font size="4">RESEARCH</font></h4>
      <p class="tag">
        <small>
          I work on computer vision, and in order to "understand" the visual world, work at the intersection of <strong>vision and language</strong>. 
        More specifically, I am interested in the following questions:
        <ul class="tag">
          <li> How can we incorporate semantic concepts from <strong>language for assigning meaning to visible </strong> things ? 
            <!-- <span style="float:right;"><i align="right">[the "crux"]</i></span> -->
          </li>
          <li> How can we build robust models that can reason about visuals and text? <span style="float:right;"><i> [the "thesis"]</i> </span></li>
          <li> Why should we build robust V&L models? Cui bono? Big-Tech surveillance state? <span style="float:right;"><i>[recurring moral conflict]</i> </span></li>
          <li> What is so special about language? Can we assign meaning and learn relationships <strong> without language? </strong> <span style="float:right;"><i>[long-term open question]</i></span> </li>
        </ul>
      </small>
      </p>
      <h4>&nbsp;</h4>


      <!-- Team -->
      <h4><font size="4">PEOPLE</font></h4>
      <h4><font size="2">Collaborators</font></h4>
      <small>
        <ul class="tag"> 
          <li> <strong>(ASU)</strong> &nbsp;&nbsp;&nbsp;&nbsp;
            <!-- <span style="float:right;"> -->
              <a href="http://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>, 
              <a href="https://pratyay-banerjee.github.io">Pratyay Banerjee</a>, 
              <a href="https://shailaja-sampat.mystrikingly.com/">Shailaja Sampat</a>
            <!-- </span> -->
          </li>
          <li><strong>(CMU/Adobe)</strong>&nbsp;&nbsp;&nbsp;&nbsp;
            <!-- <span style="float:right;">-->
              <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>
            <!-- </span> -->
          </li>
          <li><strong>(LLNL)</strong>&nbsp;&nbsp;&nbsp;&nbsp;
            <!-- <span style="float:right;"> -->
              <a href="https://rushila.com/">Rushil Anirudh</a>, 
              <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
              <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>
            <!-- </span> -->
          </li>
        </ul>
      </small>

      <h4><font size="2">Mentees</font></h4>
      <small>
        <ul class="tag">
          <li><strong>(MS Research)</strong> 
            Shivakshit Patri (2019 Spring), 
            <a href="https://www.linkedin.com/in/aadhavan-sadasivam/">Aadhavan Sadasivam</a> (2019-20), 
            Itay Goldshmid (2020 Summer),
            Abhishek Chaudhary (2020 Summer - 2021 Spring), 
            Arnav Chakravarthy (2020 Fall - 2021 Spring)
          </li>

          <li><strong><a href="https://furi.engineering.asu.edu/">(ASU FURI)</a></strong> <a href="https://www.linkedin.com/in/mertay-dayanc/">Mertay Dayanc</a></li>

          <li><strong>(BS Capstone) 2019-20</strong>
            <a href="https://www.linkedin.com/in/paulfbutler2016">Paul Butler</a> (Video QA), 
            Jace Lord (Image Rendering), 
            Sagarika Pannase, 
            William Tith, 
            Aashwin Ranjan (Visual Reasoning)
          </li>
        </ul>
      </small>

      <h4><font size="2">Friends who "Science"</font></h4>
      <small>
        <ul class="tag">
          <li><strong>(ASU)</strong>                
            <a href="https://kowshikthopalli.github.io/">Kowshik Thopalli</a>,          
            <a href="https://www.public.asu.edu/~lcheng35/">Lu Cheng</a>, 
            <!-- <a href="https://www.linkedin.com/in/aurghob/">Aurgho Bhattacharjee</a>,  -->
            <!-- <a href="https://aadhavansadasivam.com/">Aadhavan Sadasivam</a>,  -->
            <a href="http://samrawal.com/">Sam Rawal</a>, 
            <a href="https://www.linkedin.com/in/john-janiczek/">John Janiczek</a>, 
            <a href="https://andburch.github.io/">Andrew Burchill</a>, 
            <a href="https://scholar.google.com/citations?user=DXQIKGUAAAAJ">Tyler Quigley</a>, 
            <a href="https://www.taeyeongchoi.com/">Taeyeong Choi</a>, 
            <a href="https://www.linkedin.com/in/man-luo-a7aa57178/">Man Luo</a>
          </li>
          <li><strong>(CMU)</strong> 
            <a href="https://users.ece.cmu.edu/~vsaragad/">Vishwanath Saragadam</a>, 
            <!-- <a href="https://scholar.google.com/citations?user=LNXNkLEAAAAJ">Malhar Chaudhari</a>,  -->
            <a href="https://www.semanticscholar.org/author/Mihir-Hasabnis/103233313">Mihir Hasabnis</a>, 
            <a href="https://www.linkedin.com/in/bhargavghanekar">Bhargav Ghanekar </a>, 
            <a href="https://rachel-sunrui.github.io/"> Rachel Sun </a>
          </li>
          <li><strong>(BITS)</strong>
            <a href="https://scholar.google.com/citations?user=WyVypxAAAAAJ">Ninad Kanetkar (NEU BioEng)</a>, 
            <a href="http://ishankhurz.mystrikingly.com/">Ishan Khurjekar (UF ECE)</a>, 
            <a href="">Aseem Pradhan (GMU BioEng)</a>
          </li>
        </ul>
      </small>
      <h4>&nbsp;</h4>


      <h4><font size="4">SERVICE</font></h4>
      <small>
        <ul class="tag">
          <li><strong>Reviewer:</strong> 
            <br>&emsp;&emsp;<a href="https://2021.naacl.org/">NAACL 2021</a>,
            <br>&emsp;&emsp;<a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>,
            <a href="https://aaai.org/Conferences/AAAI-21/aaai21demoscall/">AAAI 2021 Demos</a>,  
            <br>&emsp;&emsp;<a href="https://www.icra2019.org/">ICRA 2019</a>, <a href="https://www.icra2020.org/">ICRA 2020</a>, <a href="http://www.icra2021.org/">ICRA 2021</a> / <a href="https://www.ieee-ras.org/publications/ra-l">IEEE RA-L</a>, 
            <br>&emsp;&emsp;<a href="https://www.springer.com/journal/138">Springer MVAP</a> 
          </li>
          <li><strong>Organizer / Host:</strong>
            <br>&emsp;&emsp;<a href="https://asu-active-perception-group.github.io/seminar/">Spring 2021 Seminar Series (Frontiers in Vision and Language)</a>
            <br>&emsp;&emsp;<a href="reading_group.html">Summer Vision Reading Group</a>
          <li><strong>Volunteer:</strong>
            <a href="https://icml.cc/Conferences/2020">ICML 2020</a>,
            <a href="https://swrobotics.engineering.asu.edu/wp-content/uploads/2019/10/SWRS_Program_2019_final.pdf">SWRS 2019</a>
          </li>
          <li><strong>Research Mentor:</strong> ASU FURI, CSE485 Capstone (Cognitive Vision, Vision&Language)</li>
          <li><strong>Teaching:</strong>
            <br>&emsp;&emsp;ASU CSE310: Data Structures and Algorithms (Taught Recitations) 
            <br>&emsp;&emsp;ASU CSE408: Multimedia Information Systems (TA)
            <br>&emsp;&emsp;ASU CSE110: Principles of Programming (Taught Labs)
            <br>&emsp;&emsp;ASU CSE576: Natural Language Processing (Mentored Class Projects)
            <br>&emsp;&emsp;BITS CTE: Advanced Image Processing (co-Instructor)
          </li>
          <li><strong>Student Mentor:</strong> 
            <br>&emsp;&emsp;Graduate Student Mentorship Program (GSMP), ASU, 2019-present.
            <br>&emsp;&emsp;Peer Mentorship Program (first installment), BITS Goa, 2014
          </li>
        </ul>          
        </small>


      <h4>&nbsp;</h4> 
    </div>


    <div class="thumbnail2">
      <!-- Affiliations -->
      <h4><font size="4">AFFILIATIONS</font></h4>
      <p class="tag">
        <table align="center">
          <tr>
            <td align="center"><a href="https://www.llnl.gov/"><img src="images/llnl_logo.jpg" height="90px" class="rounded-corner"></a></td>
            <td align="center"><a href="http://www.asu.edu"><img src="images/asu_logo.png" height="90px" class="papericon"></a></td>
            <td align="center"><a href="http://www.cmu.edu"><img src="images/cmu_logo.png" height="90px" class="papericon"></a></td>
            <td align="center"><a href="https://research.snap.com/"><img src="images/snap_logo.png" height="90px" class="papericon"></a></td>
            <td align="center"><a href="https://www.bits-pilani.ac.in/"><img src="images/bits_logo.png" height="90px" class="papericon"></a></td>
            <td align="center"><a href="http://hkn.ece.cmu.edu/"><img src="images/hkn_logo.png" height="90px" class="papericon"></a></td>
          </tr>
          <tr>
            <td align="center"><small><strong>Lawrence Livermore <br>National Laboratory</strong><br />Summer 2020, 2021</small></td>
            <td align="center"><small><strong>Arizona State <br>University</strong><br />2018-present</small></td>
            <td align="center"><small><strong>Carnegie Mellon <br>University</strong><br />2016-2018</small></td>
            <td align="center"><small><strong>Snap Inc. <br>Research</strong><br />Summer 2018</small></td>
            <td align="center"><small><strong>BITS <br>Pilani<br /></strong>2011-2015</small></td>
            <td align="center"><small><strong>IEEE <br>Eta Kappa Nu</strong><br />2017-present</small></td>
          </tr>
        </table>
      </p>
      <h4>&nbsp;</h4> 
      <!-- Publication List -->
      <h4><font size="4">SELECTED PUBLICATIONS</font></h4>
      <small>
      <table>
          <tbody>
            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td>
                  <a href="images/agat_overall_new.png"><img src="images/agat_overall_new.png" class="teaser"></a>
                  <p>
                    <a href="https://arxiv.org/pdf/2012.01806.pdf">Paper</a>
                  </p>
              </td>

              <td class="panel">
                <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
                  Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
                </a>
                <br/><venue>AAAI 2021</venue>
                <br>
                <strong>Tejas Gokhale</strong>,
                <author><a href="https://rushila.com/">Rushil Anirudh</a>,</author>
                <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
                <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
                <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
                <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>

                <p class="tag" style="text-align:justify">
                  An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
                  Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations --- object-related shifts, geometric transformations, and common image corruptions.
                </p>
              </td>
            </tr>

            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td class="teaser">
                  <a href="images/synth_data.png"><img src="images/synth_data.png" class="teaser"></a>
                  <p>
                  <a href="https://sslneuips20.github.io/files/CameraReadys%203-77/33/CameraReady/neurips_workshop_ssl_camready.pdf">Workshop Paper</a>, <a href="docs/ss_vqa_preprint.pdf">Full Version</a>
                </p>
              </td>

              <td class="panel">
                <a href="https://arxiv.org/abs/2012.02356" class="papertitle">Self-Supervised VQA: Answering Visual Questions using Images and Captions
                </a>
                <br/><venue>NeurIPS 2020 SSL Workshop</venue>
                <br>
                <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
                <strong>Tejas Gokhale</strong>,
                <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
                <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>

                
                <p class="tag" style="text-align:justify">
                  We study whether models can be trained without any human-annotated Q-A pairs, butonly with images and associated text captions which are descriptive  and  less  subjective. 
                  We propose  spatial-pyramid  image  patches  as  a  simple  but  effective alternative to object bounding boxes, and demonstrate that our method uses fewer human annotations.
                </p>
              </td>
            </tr>


            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td class="one">
                  <a href="images/overall_final.png"><img src="images/overall_final.png" class="teaser"></a>
                  <p>
                    <a href="">Website (coming soon!)</a>, <a href="https://arxiv.org/abs/2009.08566">Paper</a>
                  </p>
                </td>

                <td style="padding:20px;width:55%;vertical-align:top">
                  <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
                  </a>
                  <br/><venue>EMNLP 2020</venue>
                  <br>
                  <strong>Tejas Gokhale*</strong>,
                  <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
                  <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
                  <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
                  
                  <p class="tag" style="text-align:justify">
                    MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
                    We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
                    MUTANT establishes a new SOTA (+10%) on the VQA-CP challenge (for generalization under Changing Priors)
                  </p>
                  
                  
                </td>
              </tr>

              <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td class="one">
                  <a href="images/v2c_tgokhale.png"><img src='images/v2c_tgokhale.png' class="teaser"></a>
                  <p>
                    <a href="https://asu-active-perception-group.github.io/Video2Commonsense/">Website</a>, <a href="https://arxiv.org/pdf/2003.05162.pdf">Paper</a>
                  </p>
                </td>

                <td style="padding:20px;width:55%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
                  </a>
                  <br/><venue>EMNLP 2020</venue>
                  <br>
                  <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
                  <strong>Tejas Gokhale*</strong>,
                  <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
                  <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
                  <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
                  
                  <p class="tag" style="text-align:justify">
                    Actions in videos are inherently linked to latent social and commonsense aspects.
                    We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
                    <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
                    Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
                  </p>
                  
                  
                </td>
              </tr>

              <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td class="teaser">
                  <a href="images/lol_tgokhale.png"><img src='images/lol_tgokhale.png' class="teaser"></a>
                  <p>
                    <a href="./vqa_lol.html">Website</a>, <a href="https://arxiv.org/pdf/2002.08325.pdf">Paper</a>, <a href="https://youtu.be/u6dEKvwla9M">Video</a>
                  </p>
                </td>


                <td style="padding:20px;width:55%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
                  </a>
                  <br><venue>ECCV 2020</venue>
                  <br>
                  <strong>Tejas Gokhale*</strong>,
                  <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
                  <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
                  <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
                  

                  <p class="tag" style="text-align:justify">
                    VQA models struggle at negation, antonyms, conjunction, disjunction!
                    We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
                  </p>
                  
                </td>
              </tr>

              <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                <td class="one">
                  <a href="images/teaser_ext.png"><img src='images/teaser_ext.png' class="teaser"></a>
                  <p>
                    <a href="https://arxiv.org/pdf/2004.08614.pdf">Paper</a>
                  </p>
                </td>
                <td class="panel">
                  <description> 

                    <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
                    <br/><venue class="arxiv">preprint, arxiv 2020</venue>
                    <br>
                    <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
                    <strong>Tejas Gokhale</strong>,
                    <a href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
                    <a href="https://pavanturaga.com/">Pavan Turaga</a>,
                    <a href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
                    <p class="tag" style="text-align:justify">             
                      Scene completion from sparse and incomplete label maps.
                      `Halluci-Net' is a 2-stage method that 
                      captures the object co-occurrence relationships,
                      to produce dense label maps from incomplete labelmaps and object boundaries,
                      for image synthesis.
                      <!-- HalluciNet outperforms single-stage baseline me thods on various performance metrics. -->
                    </p>
                    
                  </td>
                </tr>

                <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
                  <td class="one">
                    <a href="images/blocksworld_tgokhale.PNG"><img src='images/blocksworld_tgokhale.PNG' class="teaser"></a>
                    <p><a href="https://asu-active-perception-group.github.io/bird_dataset_web/">Website</a>, <a href="https://arxiv.org/pdf/1905.12042.pdf">Longer Preprint</a>, <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf">CVPR-VMC Paper</a></p>

                  </td>


                  <td style="padding:20px;width:55%;vertical-align:top">
                    <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="papertitle">Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs
                    </a>
                    <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
                    <!-- <p>&nbsp;</p>
                    <a href="https://arxiv.org/pdf/1905.12042.pdf" class="papertitle">Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs
                    </a> -->
                    <!-- <br/><venue class="arxiv">(longer preprint version)</venue>, arxiv 2019 -->
                    <br><strong>Tejas Gokhale</strong>,
                    <a href="">Shailaja Sampat</a>,
                    <a href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
                    <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
                    <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
                  </div>

                  
                  <p class="tag" style="text-align:justify">
                    Given two images (source, target) with different object configurations, 
                    what is the sequence of steps to re-arrange source to match target?
                    For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
                    and exhibits inductive generalization.
                  </p>

                </td>
              </tr>
          </tbody>
        </table>
    </div> 

    <div class="thumbnail">
      <!-- News --> 
      <h4><font size="4">NEWS</font></h4>
      <p class="tag"><small>
        <strong><font color="000000">Mar 2021:</font></strong>
        "Self-Supervised Test-Time Learning for Reading Comprehension" -- work pioneered by <a href="https://pratyay-banerjee.github.io/">Pratyay</a> accepted to NAACL 2021! Preprint coming soon.
      </small></p>
      <p class="tag"><small>
        <strong><font color="000000">Jan 2021:</font></strong>
        Co-organizing the <a href="https://asu-active-perception-group.github.io/seminar/">ASU-APG Seminar Series </a> on "Frontier Topics in Vision and/or Language" in Spring 2021.
      </small></p>
      <p class="tag"><small>
        <strong><font color="000000">Dec 2020:</font></strong>
        <a href="https://arxiv.org/abs/2012.01806">AGAT (Attribute Guided Adversarial Training)</a> was accepted to AAAI 2021. <a href="https://github.com/ASU-Active-Perception-Group/vqa_lol"><font color="maroon">Dataset</font></a>
      </small></p>
      <p class="tag"><small>
        <strong><font color="000000">Sep 2020:</font></strong>
        <a href="https://arxiv.org/abs/2009.08566">VQA-MUTANT</a> was accepted to EMNLP 2020. </a> <a href="https://github.com/ASU-Active-Perception-Group/vqa_mutant"><font color="maroon">Code</font></a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Oct 2020:</font></strong>
        Hosted the first "Season" of <a href="./reading_group.html"> Summer Vision Reading Group </a> at ASU. (May-Oct 2020)
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Sep 2020:</font></strong>
        <a href="https://arxiv.org/pdf/2003.05162.pdf">V2C (Video to Commonsense)</a> was accepted to EMNLP 2020 <a href="https://asu-active-perception-group.github.io/Video2Commonsense/">Website</a>, <a href="https://github.com/jacobswan1/Video_S3D_Talker"><font color="maroon">Code</font></a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jul 2020:</font></strong>
        <a href="https://arxiv.org/pdf/2002.08325.pdf">VQA-LOL: VQA under the Lens of Logic</a> accepted to ECCV 2020! <a href="https://youtu.be/u6dEKvwla9M"><font color="magenta">Talk</font></a>, <a href="./vqa_lol.html">Website</a>, <a href="https://github.com/ASU-Active-Perception-Group/vqa_lol"><font color="maroon">Code</font></a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">May 2020:</font></strong>
        Internship at Lawrence Livermore National Labs mentored by <a href="https://www.rushila.com">Rushil Anirudh</a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Apr 2020:</font></strong>
        Halluci-Net (work done at CMU) <a href="https://arxiv.org/pdf/2004.08614.pdf">Preprint</a> is now available!
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Mar 2020:</font></strong>
        I was awarded the CIDSE Doctoral Fellowship and the Engineering Graduate Fellowship.</a>.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Aug 2019:</font></strong>
        "Vision beyond Pixels", <i><a href='./presentations/tgokhale_ijcai2019dc.pdf'> <font color="magenta">Talk</font> </a></i> at IJCAI 2019 Doctoral Consortium, <i><font color="ForestGreen">Received IJCAI DC Travel Award</font></i>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jul 2019:</font></strong>
        "Reasoning about Objects and Actions via Block-Play", <font color="magenta">Talk</font> at Telluride 2019 <a href="https://sites.google.com/view/telluride2019/home">Neuromorphic Cognition Engineering Workshop</a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jun 2019:</font></strong>
        "Cooking with Blocks", Workshop paper presented at <a href="http://cvpr2019.thecvf.com/">CVPR</a> 2019, <font color="ForestGreen"><i>Received CIDSE Travel Grant for attending</i>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Aug 2018:</font></strong>
        Joined <a href="https://yezhouyang.engineering.asu.edu/research-group/">ASU Active Perception Group </a> as a PhD student with Yezhou Yang. Co-advised by Chitta Baral.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">May 2018:</font></strong>
        Internship at Snap Research, Seattle with <a href="https://www.linkedin.com/in/krishnanguru/">Guru Krishnan</a> and <a href="http://www.cs.columbia.edu/~nayar/">Shree Nayar</a>
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Apr 2018:</font></strong>
        "Deep Learning Methods in Imaging and Computer Vision", <i><a href="https://touch.facebook.com/AlumniRelationsBITSGoa/photos/a.577537568978007/1797680853630333/?type=3&source=54"> <font color="magenta">Invited Talk</font></a></i> at BITS Pilani.
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Dec 2017:</font></strong>
        Graduated with M.S. (ECE) from <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a> (with <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu </a> Honors) 
      </small></p>

      <p class="tag"><small>
        <strong><font color="000000">Jan 2017:</font></strong>
        Joined <a href="http://imagesci.ece.cmu.edu/">Image Science Lab</a> at CMU as a Graduate Researcher with Aswin Sankaranarayanan.
      </small></p>

      <h4>&nbsp;</h4>


      <h4><font size="4">AWARDS and HONORS</font></h4>
      <h4><font size="2">Scholarships / Fellowships</font></h4>
      <ul class="tag">
        <li> <a href="http://www.ncert.nic.in/programmes/talent_exam/index_talent.html">National Talent Scholarship</a>, (Govt. of India, 2007-2015)</li>
        <li> Engineering Graduate Fellowship</a> (ASU Engineering, Spring 2020)</li>
        <li> <a href="https://graduate.asu.edu/current-students/funding-opportunities/awards-and-fellowships/university-graduate-fellowships"> CIDSE Doctoral Fellowship</a> (CIDSE, ASU, Spring 2020)</li>
      </ul>

      <h4><font size="2">Travel Awards</font></h4>
      <ul class="tag">
        <li>Graduate College Travel Award, ASU (for EMNLP 2020)</li>
        <li>Graduate College Travel Award, ASU (for ECCV 2020)</li>
        <li> IJCAI Doctoral Consortium Travel Award, (IJCAI, 2019)</li>
        <li> CIDSE Travel Grant Award, (for CVPR 2019) </li>
      </ul>

      <h4><font size="2">Societies / Memberships</font></h4>
      <ul class="tag">
        <li> Inducted, <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu Sigma Chapter</a> (Carnegie Mellon University, 2017)</li>
        <li> Member, <a href="https://www.thecvf.com/">Computer Vision Foundation</a></li>
        <li> Student Member, <a href="https://www.aclweb.org/portal/">Association for Computational Linguistics</a></li>
        <li> Student Member, <a href="https://aaai.org/">Association for the Advancement of Artificial Intelligence</a></li> 
      </ul>           


    </div>
  </div>
</body>
</html>

