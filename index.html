
<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tejas Gokhale</title>
    <meta charset="utf-8" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <!-- integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" -->
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="theme.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
<body>
  
<div class="container" style="background-color: #fcfcfc;">
<div class="container">
  <div class="row mb-3 justify-content-center">
    <div class="col-xs-5 col-md-5">
      <center>
        <br>
        <h1><b>Tejas Gokhale</b></h1>
        PhD Candidate<br />
        <br /><a href="https://scai.engineering.asu.edu/">School of Computing & AI</a>, Arizona State University
        <br/>
        tgokhale@asu.edu<br /><br />

        <a class="button btn btn-primary btn-sm mb-1" href="#about">about</a>
        <!-- <a class="button btn btn-primary btn-sm mb-1" href="#news">news</a> -->
        <a class="button btn btn-primary btn-sm mb-1" href="#publications">publications</a>
        <!-- <a class="button btn btn-primary btn-sm mb-1" href="#misc">misc.</a> -->
        <a class="button btn btn-primary btn-sm mb-1" href="tgokhale_resume.pdf">CV (pdf)</a> 
        <a class="button btn btn-primary btn-sm mb-1" href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ">Google Scholar</a> 
        <a class="button btn btn-primary btn-sm mb-1" href="https://asu-apg.github.io/seminar/">V&L Seminar</a> 
        <a class="button btn btn-primary btn-sm mb-1" href="https://asu-apg.github.io/odrum/">O-DRUM</a> 
        <a class="button btn btn-primary btn-sm mb-1" href="./reading_group.html">Reading Group</a> 

      </center>

    </div>
    <div class="col-xs-4 col-md-4">
      <center>
        <br />
        <img src="images/tg_okc.jpg" style="max-width: 256px;" alt="Tejas Gokhale"/>
        <!-- <img alt="A random picture either of me or that I took. Click photo to view the entire collection.", id="photo", width="200", height="200" /><br /><br />
        <script>
        index = Math.floor(Math.random() * 31);
        document.getElementById("photo").src = "photos/photo" + index + "-small.png";
        </script> -->
      </center> 
    </div>
  </div>
</div>
<hr />
<div class="container">
  <div class="row justify-content-left">
    <div class="col-md-8 mb-3">
      <a id="about"><h3>About</h3></a>
      <p>
        I work with 
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> and
        <a href="http://www.public.asu.edu/~cbaral/">Chitta Baral</a> at ASU, 
        and closely collaborate with 
        <a href="https://rushila.com">Rushil Anirudh</a> at
        <a href="https://computing.llnl.gov/">Lawrence Livermore National Laboratories</a>.
        I received my MS in ECE from 
        <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a>, 
        where I worked with 
        <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>. 
      </p>
      <p>
        I work on <b>computer vision</b>, <b>machine learning</b>, and <b>natural language processing</b> -- very often at the wonderful inetrsection of the three disciplines.
        My domain expertise lies in <b>"semantic vision"</b>, i.e. computer vision tasks that seek to assign "meaning" to what we see -- this includes <i>"designative"</i> tasks such as image classification; <i>"communicative"</i> tasks involving both vision and language such as visual question answering, visual reasoning, and image captioning.
      </p>
      <p>
        The main focus of my Ph.D. is on <b>robust visual understanding</b>, to address problems such as domain shift/out-of-distribution generalization, linguistic robustness (logical, semantic), visual robustness (corruptions, geometric transformations, attribute-level shift). 
        <!-- 
        My thesis takes the following approach to tackle robustness and generalization in semantic vision:
        <ol>
          <li><b>Identify failure modes</b> â€“ situations under which computer vision systems may fail for semantic vision tasks, and <b>design analytical tools</b> such as datasets, protocols, and metrics for evaluation </li>
          <li>Develop <b>machine learning algorithms</b> and data transformation methods that mitigate the risks posed by such situations, </li> 
          <li>Develop <b>robust optimization algorithms</b> to <b>discover data transformations</b> that mitigate the risks posed by such situations</li>
          -->
        </ol> 
      </p>

      <hr class="new1" />
      <a id="news"><h3>News</h3></a>
      <p><span class="badge badge-primary badge-pill">20 Jun 2022</span> Workshop at <a href="https://asu-apg.github.io/odrum/">CVPR 2022</a></p>
      <p><span class="badge badge-primary badge-pill">25 Apr 2022</span> Recognized as <a href="https://iclr.cc/Conferences/2022/Reviewers">Highlighted Reviewer for ICLR 2022 </a>(top ~8%)</p>
      <p><span class="badge badge-primary badge-pill">30 Mar 2022</span> Guest Lecture for <a href="">CSE598 (Perception in Robotics)</a>, ASU</p>
      <br/>
    </div>

    <div class="col-xs-4 col-md-4 mb-3">
      <div class="card rounded bg-success" style="min-height:160px;">
        <div class="card-body text-black">
          <div class="card-title">Hosting the 1st Workshop on Open-Domain Retrieval Under Multi-Modal Settings <a href="https://asu-apg.github.io/odrum/" >(O-DRUM)</a> at CVPR 2022!</div>

          <div class="card-title">I will present my research at <a href="https://cvpr2022.thecvf.com/doctoral-consortium" >CVPR Doctoral Consortium</a></div>
          <div class="card-text"><small>
            <ul>
              <li>A <a href="./docs/tgokhale_rs_cvpr_dc.pdf" >Research Statement</a> and a <a href="./docs/tgokhale_poster_dc.pdf" >poster</a> summarizing some of my work, is now available. </li>
            </ul>
          </small></div>
          <!-- <a class="button btn btn-dark btn-sm mb-1" href="docs/tgokhale_rs.pdf">research statement</a> -->
          <!-- <p class="card-text"><small>*These materials are general, so might be slightly different than what I submitted in each application.</small></p> -->
        </div>
      </div>
      <br/>
      <div class="card rounded bg-success" style="min-height:160px;">
        <div class="card-body text-black">
          <small>
          <b>Collaboration/Mentorship Opportunities:</b>
          I am always happy to discuss new research ideas; if you're a PhD student interested on collaborating on robust machine learning, domain generalization, robust visual and textual understanding or other related topics, please send me an email (or grab a coffee with me if you're at ASU).
          I am also available to give advice and share my experiences w.r.t. admissions to Ph.D. programs in CS/EE/CE.
          <!-- I typically mentor and collaborate with one or two MS thesis students at ASU every year; for such collaborations, as a first step, I recommend pitching your ideas to Yezhou Yang and formally signing him up on your thesis committee. -->
          </small>
        </div>
      </div>
    </div>
  </div>
</div>



<div class="container">
  <div class="row justify-content-left">

    <div class="col-md-12 mb-3">
      <a id="affiliations"><h3>Affiliations</h3></a>
      <table align="justify">
        <tr align="justify">
          <td align="center">
            <a href="https://www.microsoft.com/en-us/research/">
              &nbsp;&nbsp;<img src="images/msr_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>Microsoft </br> Research </b><br />Summer 2022
          </td>
          <td align="center">
            <a href="https://www.llnl.gov/">
              &nbsp;&nbsp;<img src="images/llnl_logo.jpg" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>Lawrence Livermore <br>National Laboratory</b><br />Summer 2020, 2021
          </td>
          <td align="center">  
            <a href="http://www.asu.edu">
              &nbsp;&nbsp;<img src="images/asu_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>Arizona State <br>University</b><br />2018-present
          </td>
          <td align="center">
            <a href="http://www.cmu.edu">
              &nbsp;&nbsp;<img src="images/cmu_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>Carnegie Mellon <br>University</b><br />2016-2018
          </td>
        <!-- </tr> -->
        <!-- <tr align="justify"> -->
          <td align="center">
            <a href="https://research.snap.com/">
              &nbsp;&nbsp;<img src="images/snap_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>Snap Inc. <br>Research</b><br />Summer 2018
          </td>
          <td align="center">
            <a href="https://www.bits-pilani.ac.in/">
              &nbsp;&nbsp;<img src="images/bits_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>BITS <br>Pilani<br /></b>2011-2015
          </td>
          <td align="center">
            <a href="http://hkn.ece.cmu.edu/">
              &nbsp;&nbsp;<img src="images/hkn_logo.png" class="logo" style="max-width:100px;max-height:100px;">&nbsp;&nbsp;
            </a>
            <br/><b>IEEE <br>Eta Kappa Nu</b><br />2017-present
          </td>
        </tr>
      </table>
    </div>    
  </div>
</div>



<hr />
<div class="container">
  <div class="row justify-content-left">
    <div class="col-xs-12 col-md-12 mb-3">
      <a id="publications"><h3>Publications</h3></a>
      <small>* indicates equal contribution</small><br /><br />

      <!--       <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" style="text-decoration:none;color:inherit;"><img src=""></a>
        </div>
        <div class="col-md-8 mb-3">
          
        </div>
      </div> -->

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" style="text-decoration:none;color:inherit;"><img src="images/alt_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2206.07736" class="papertitle">Improving Diversity with Adversarially Learned Transformations for Domain Generalization</a>
          <br/><venue>(to appear in) WACV 2023</venue>
          <br>
          <b>Tejas Gokhale</b>,
          <a href="https://rushila.com/">Rushil Anirudh</a>,
          <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
          <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2206.07736.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/ALT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>

          <p class="tag" style="text-align:justify">
            ALT discovers diverse and adversarial transformations using an image-to-image neural network with learnable weights.
            ALT improves the state-of-the-art single domain generalization performance on three benchmarks and is significantly better than pixel-wise adversarial training and standard data augmentation techniques.
          </p>
        </div>
      </div>


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" style="text-decoration:none;color:inherit;"><img src="images/example_nlvr_violin.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2110.07165" class="papertitle">Semantically Distributed Robust Optimization for Vision-and-Language Inference</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b>Tejas Gokhale</b>,
          Abhishek Chaudhary, 
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/pdf/2110.07165.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            SDRO: a distributed robust optimization method that operates with linguistic transformations of sentence inputs, SISP: a suit of semantics-inverting (SI) and semantics-preserving (SP) linguistic transformations, and an ensembling technique for vision-and-language inference.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <!-- <a href="" style="text-decoration:none;color:inherit;"><img src=""></a> -->
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2203.07653" class="papertitle"><i>Generalized but not Robust?</i> Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b>Tejas Gokhale</b>, 
          <a href="https://luomancs.github.io/">Man Luo</a>, 
          <a href="https://scholar.google.com/citations?user=-7LK2SwAAAAJ">Swaroop Mishra</a>,
          <a href="https://www.linkedin.com/in/bhavdeep-singh-sachdeva-767a3b7a">Bhavdeep Singh Sachdeva</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2203.07653.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).
            This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" style="text-decoration:none;color:inherit;"><img src="images/bionr_qg.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2201.07745" class="papertitle">Improving Biomedical Information Retrieval with Neural Retrievers</a>
          <br/><venue>AAAI 2022</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Man Luo</a>,
          <a href=""> Arindam Mitra </a>,
          <b>Tejas Gokhale</b>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2201.07745.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            We seek to improve information retrieval (IR) using neural retrievers (NR) in the biomedical domain, using a three-pronged approach. (1) a template-based question generation method, (2) two novel pre-training tasks that are closely aligned to the downstream task of information retrieval, (3) the ``Poly-DPR'' model which encodes each context into multiple context vectors. 
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="" style="text-decoration:none;color:inherit;"><img src="images/spatial_vqa.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2109.01934" class="papertitle">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering</a>
          <br/><venue>ICCV 2021</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2109.01934.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            VQA models trained with two additional objectives: object centroid estimation and relative position estimation, lead to improved performance on spatial reasoning questions (in GQA) in fully supervised and few shot settings as well as improved O.O.D. generalization.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/synth_data.png"style="text-decoration:none;color:inherit;"><img src="images/synth_data.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2012.02356" class="papertitle">WeaQA: Weak Supervision via Captions for Visual Question Answering</a>
          <br/><venue>ACL 2021 Findings</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.02356.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            We show that models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions.  Our experiments suggest gains on benchmark with shifted priors (VQA-CP) over baselines which use full supervision from human-authored QA data.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/teaser_ext.png"style="text-decoration:none;color:inherit;"><img src="images/teaser_ext.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
          <br/><venue class="arxiv">CVPR 2021 Workshop</venue>, "AI for Content Creation"
          <br>
          <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
          <b>Tejas Gokhale</b>,
          <a href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
          <a href="https://pavanturaga.com/">Pavan Turaga</a>,
          <a href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
          <br>
          <a href="https://arxiv.org/pdf/2004.08614.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">             
            Scene completion from sparse and incomplete label maps.
            `Halluci-Net' is a 2-stage method that 
            captures the object co-occurrence relationships,
            to produce dense label maps from incomplete labelmaps and object boundaries,
            for image synthesis.
          </p>
        </div>
      </div>


      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/ttl_rc.png"style="text-decoration:none;color:inherit;"><img src="images/ttl_rc.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2103.11263" >
            Self-Supervised Test-Time Learning for Reading Comprehension
          </a>
          <br/><venue>NAACL 2021</venue>
          <br>
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b>Tejas Gokhale</b>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2103.11263.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>

          <p class="tag" style="text-align:justify">
            Unsupervised Reading Comprehension method that operates directly on a single test passage.
            Synthetic QA pairs are generated from the passage, and models are trained on these.
            When a new human-authored test question appears, models infer answers better than previous unsupervised methods.
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/agat_overall_new.png"style="text-decoration:none;color:inherit;"><img src="images/agat_overall_new.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
            Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
          </a>
          <br/><venue>AAAI 2021</venue>
          <br>
          <b>Tejas Gokhale</b>,
          <a href="https://rushila.com/">Rushil Anirudh</a>,
          <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.01806.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/AGAT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>

          <p class="tag" style="text-align:justify">
            An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
            Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations --- object-related shifts, geometric transformations, and common image corruptions.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/overall_final.png"style="text-decoration:none;color:inherit;"><img src="images/overall_final.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/abs/2009.08566" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag" style="text-align:justify">
            MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
            We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
            MUTANT establishes a new SOTA (+10%) on the VQA-CP challenge (for generalization under Changing Priors)
          </p>            
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/v2c_tgokhale.png"style="text-decoration:none;color:inherit;"><img src="images/v2c_tgokhale.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/pdf/2003.05162.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://asu-active-perception-group.github.io/Video2Commonsense/" class="badge bg-white badge-sm text-decoration-none">web</a>
          <p class="tag" style="text-align:justify">
            Actions in videos are inherently linked to latent social and commonsense aspects.
            We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
            <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
            Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
          </p>      
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/lol_tgokhale.png"style="text-decoration:none;color:inherit;"><img src="images/lol_tgokhale.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
          </a>
          <br><venue>ECCV 2020</venue>
          <br>
          <b>Tejas Gokhale*</b>,
          <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
          <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          
          <a href="https://arxiv.org/pdf/2002.08325.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>, 
          <a href="./vqa_lol.html" class="badge bg-white badge-sm text-decoration-none">web</a>
          <a href="https://youtu.be/u6dEKvwla9M" class="badge bg-white badge-sm text-decoration-none">video</a>
          <p class="tag" style="text-align:justify">
            VQA models struggle at negation, antonyms, conjunction, disjunction!
            We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
          </p>
        </div>
      </div>

      <div class="row mb-3 rounded justify-content-center bg-light">
        <div class="col-xs-4 col-md-4 mb-3">
          <a href="images/blocksworld_tgokhale.PNG"style="text-decoration:none;color:inherit;"><img src="images/blocksworld_tgokhale.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="papertitle">Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs
          </a>
          <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
          <br><b>Tejas Gokhale</b>,
          <a href="">Shailaja Sampat</a>,
          <a href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <br>
          <a href="https://arxiv.org/pdf/1905.12042.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>, 
          <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">[CVPR-VMC Paper]</a>
          <a href="https://asu-active-perception-group.github.io/bird_dataset_web/" class="badge bg-white badge-sm text-decoration-none">web</a>
          <p class="tag" style="text-align:justify">
            Given two images (source, target) with different object configurations, 
            what is the sequence of steps to re-arrange source to match target?
            For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
            and exhibits inductive generalization.
          </p>
        </div>
      </div>
  </div>
</div>
<div>
  <div class="row justify-content-left">
    <div class="col-md-12 mb-3">
      <a id="awards"><h3>Awards</h3></a>
      ICLR 2022 <a href="https://iclr.cc/Conferences/2022/Reviewers">Highlighted Reviewer</a><br>
      CVPR 2022 <a href="https://cvpr2022.thecvf.com/doctoral-consortium">Doctoral Consortium </a><br>
      IJCAI 2019 <a href="https://www.ijcai19.org/doctoral-consortium.html"> Doctoral Consortium</a><br>

      <b>Scholarships / Fellowships:</b>
      <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <li> <a href="https://graduate.asu.edu/current-students/funding-opportunities/awards-and-fellowships/university-graduate-fellowships"> SCAI Doctoral Fellowship</a> (ASU, Spring 2020, Spring 2021, Spring 2022)</li>
        <li> Engineering Graduate Fellowship</a> (ASU, Spring 2020)</li>
        <li> <a href="http://www.ncert.nic.in/programmes/talent_exam/index_talent.html">National Talent Scholarship</a>, (Govt. of India, 2007-2015)</li>
      </ul>
      
      <b>Travel Awards</b>
      <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <li> Graduate College Travel Award, ASU (for CVPR 2022, ICCV 2021, EMNLP 2020, ECCV 2020)</li>
        <li> IJCAI Doctoral Consortium Travel Award, (IJCAI, 2019)</li>
        <li> CIDSE Travel Grant Award, (for CVPR 2019) </li>
      </ul>

      <b>Societies / Memberships</b>
      <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <li> Inducted, <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu Sigma Chapter</a> (Carnegie Mellon University, 2017)</li>
        <li> Member, <a href="https://www.thecvf.com/">Computer Vision Foundation</a>, <a href="https://www.aclweb.org">Association for Computational Linguistics</a>, <a href="https://aaai.org/">Association for the Advancement of Artificial Intelligence</a></li> 
      </ul> 
    </div>

    <div class="col-md-12 mb-3">
      <a id="service"><h3>Service</h3></a>
      <b>Reviewer:</b>
      <a href="https://iclr.cc/Conferences/2022"> ICLR 2022</a>, 
      <a href="https://nips.cc/Conferences/2022/"> NeurIPS 2022</a>, 
      <a href="https://eccv2022.ecva.net/"> ECCV 2022</a>, 
      AAAI <a href="https://aaai.org/Conferences/AAAI-21/">2021</a>, <a href="https://aaai.org/Conferences/AAAI-22/">2022</a>
      <a href="https://aclrollingreview.org/people">ACL Conferences / ARR (ACL, EMNLP, NAACL) 2021, 2022</a>,
<!--       <a href="https://2021.emnlp.org/">EMNLP 2021</a>,
      <a href="https://2021.naacl.org/">NAACL 2021</a>, -->
      <!-- </li> -->
      <!-- <li><i>Robotics:</i> -->
      ICRA <a href="https://www.icra2019.org/">2019</a>, <a href="https://www.icra2020.org/">2020</a>, <a href="http://www.icra2021.org/">2021</a>, <a href="https://www.ieee-ras.org/publications/ra-l">IEEE RA-L</a>, 
      <!-- </li> -->
      <!-- <li><i>Vision:</i> -->
      <a href="http://wacv2022.thecvf.com/">WACV 2022</a>, 
      <a href="https://www.springer.com/journal/138">Springer MVAP</a> 
      <br/>
      <b>Organizer/Host:</b>
      <a href="https://asu-apg.github.io/odrum/"> O-DRUM Workshop @CVPR 2022</a>, 
      <a href="https://asu-active-perception-group.github.io/seminar/">Spring 2021 Seminar Series (Frontiers in Vision and Language)</a>,
      <a href="reading_group.html">Summer Vision Reading Group</a>
      <br/>
      <b>Advisor:</b> ASU Machine Learning Club (undergraduate student organization)
      <br/>
      <b>Research Mentor:</b> ASU FURI, CSE485 Capstone (Cognitive Vision, Vision&Language)
      <br/>
      <b>Teaching:</b>
      <ul>
        <li>ASU CSE310: Data Structures and Algorithms (Taught Recitations)</li>
        <li>ASU CSE408: Multimedia Information Systems (TA),</li>
        <li>ASU CSE110: Principles of Programming (Taught Labs)</li> 
        <li>ASU CSE576: Natural Language Processing (Mentored Class Projects),</li>
        <li>BITS CTE: Advanced Image Processing (co-Instructor)</li>
      </ul>
      <b>Volunteer:</b> 
      <a href="https://icml.cc/Conferences/2020">ICML 2020</a>,
      <a href="https://swrobotics.engineering.asu.edu/wp-content/uploads/2019/10/SWRS_Program_2019_final.pdf">SWRS 2019</a>
      <br/>
      <b>Student Mentor:</b> Graduate Student Mentorship Program (ASU), Peer Mentorship Program (BITS Goa).

    </div>

    <div class="col-md-12 mb-3">
      <a id="people"><h3>People</h3></a>
      <b>Collaborators:</b>
      <ul>
        <li><i>ASU:</i> 
          <a href="http://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>, 
          <a href="https://pratyay-banerjee.github.io">Pratyay Banerjee</a>,
          <a href="https://luomancs.github.io/">Man Luo</a>, 
          <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ">Swaroop Mishra</a>, 
          <a href="https://nrjvarshney.github.io/">Neeraj Varshney</a>
        </li>
        <li>
          <i>CMU / Adobe:</i> <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>
        </li>
        <li>
          <i>LLNL:</i>
          <a href="https://rushila.com/">Rushil Anirudh</a>, 
          <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a href="https://jjthiagarajan.com/">Jay Thiagarajan</a>
        </li>
      </ul>
      <b>Mentees:</b>
      <ul>
        <li><i>MS Research</i>:
          Adela Shao (2021- ),&nbsp;
          Maitreya Patel (2021- ),&nbsp;
          <a href="https://www.linkedin.com/in/abhishek-chaudhary-56b833109">Abhishek Chaudhary</a> (2020-2021 AY) &rarr; Amazon,&nbsp;
          <a href="https://www.linkedin.com/in/arnav-chakravarthy">Arnav Chakravarthy</a> (2020-2021 AY) &rarr; VMWare,&nbsp;
          <a href="https://www.linkedin.com/in/aadhavan-sadasivam/">Aadhavan Sadasivam</a> (2019-20 AY) &&rarr; PayPal,&nbsp;
          Shivakshit Patri (2019 Spring) &rarr; Amazon
        </li>
        <li><i>Undergraduate:</i>
          <a href="https://www.linkedin.com/in/mertay-dayanc/">Mertay Dayanc</a> (ASU FURI), 
          <a href="https://www.linkedin.com/in/paulfbutler2016">Paul Butler</a> (2019-20 AY) &rarr; Microsoft, Jace Lord, Sagarika Pannase, Aashwin Ranjan, William Tith (BS Capstone).


    </div>
  </div>

</div>



<hr />
<div class="container">
  <div class="row mb-3 justify-content-center">

  <div class="col-xs-12 col-md-12 mb-3">


<hr style="border-top-color: black; margin: 0.1em auto;">
<p align='right'><i>Template borrowed from <a href="https://www.alanesuhr.com/">Alane Suhr </a>.</i></p>





<p hidden><a href=spam.html /a><p/>

</body>
</html>
