<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Tejas Gokhale">
  <!-- <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>   -->
  <title>Tejas Gokhale</title>
  <link rel="stylesheet" type="text/css" href="css/simpleGridTemplate.css">
   <!-- media="screen and (min-width: 1000px)"> -->
  <!-- <link rel='stylesheet' media='screen and (max-width: 1000px)' href='css/simpleGridTemplate_small.css' /> -->
  <link rel="icon" type="image/png" href="images/tgokhale.png">
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous"> -->


  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">

  <meta name="mobile-web-app-capable" content="yes">

  <meta property="og:locale" content="en_US">
  <meta property="og:url" content="https://tejas-gokhale.github.io/">
  <meta property="og:site_name" content="papers">
  <link rel="canonical" href="https://tejas-gokhale.github.io/">
  <link rel="stylesheet" href="./assets/css/style.css">
  <link rel="icon" href="./favicon.png" type="image/png" />
  <link rel="manifest" href="./manifest.json">
  <link rel="apple-touch-icon" sizes="180x180" href="favicon-180.png">
  <link type="application/opensearchdescription+xml" rel="search" href="./search.xml"/>
  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      /*color: #808080;*/
      /*font-weight: 400;*/
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
  </style>
</head>
<body>
  <div class="gallery">
    <div class="thumbnail" style="max-width: 1024px;">
      <table>
        <tr>
          <td style="width:20%;padding-right:0px;padding-bottom:5px" align="left">
            &nbsp;&nbsp;<a href="#"><img src="images/tg_boston.jpg" width="160" alt="Tejas Gokhale"/></a>
          </td>
          <td style="width:75%;height: 100px;padding-bottom:5px;">     
            <table>
              <tr>
                <td>
                  <h1 style="text-align:left; font-weight:800;">
                    Tejas
                    Gokhale
                  </h1>
                </td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>
                  <i>
                    rhymes with (S)tage-Us
                    <a href="https://www.youtube.com/watch?v=5fFDayUQpb0"><i class='fas fa-volume-up'></i></a>
                    Go-Clay
                    <a href="https://www.youtube.com/watch?v=lRorIh2RaSQ&t=8s"><i class='fas fa-volume-up'></i></a>
                  </i>
                </td>
              </tr>
            </table>
            <!-- My first name <b>Tejas </b><a href="https://www.youtube.com/watch?v=5fFDayUQpb0"><i class='fas fa-volume-up'></i></a> rhymes with Latin names such as <i>Albus, Marcus, Columbus, etc.</i> -->
            <!-- My family name <b>Gokhale </b><a href="https://www.youtube.com/watch?v=lRorIh2RaSQ&t=8s"><i class='fas fa-volume-up'></i></a> is approximately <i>"Go Clay"</i>. -->
            <p>
              <a href="https://cidse.engineering.asu.edu/">School of Computing & AI</a>, 
              Arizona State University
            </p>
            <!-- Contact -->
            <!-- <p class="tag">
              699 S Mill Ave. aka <a href="https://goo.gl/maps/A7JFuNGh32Pbmpv18">Brickyard Engineering</a>
            </p> -->
            <p class="tag" style="font-family:monospace;padding-bottom: 0px;"> tgokhale at asu dot edu </p>
            <p>
              <a href="https://www.public.asu.edu/~tgokhale/papers/"> Papers</a> &diams;
              <a href="https://www.semanticscholar.org/author/Tejas-Gokhale/120838645">Semantic Scholar</a> &diams;
              <a href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ">Google Scholar</a> &diams;
              <a href="./tgokhale_resume.pdf">CV (.pdf)</a> &diams;
              <!-- <br> -->
              <a href="https://asu-active-perception-group.github.io/seminar/">V&L Seminar Series</a> &diams; 
              <a href="./reading_group.html">Reading Group</a>
            </p>
          </td>
        </tr>
      </table>
      <hr/>
      <br>

      <h4><font size="4">RESEARCH</font></h4>
      
      <p>
        I work with
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
        and
        <a href="http://www.public.asu.edu/~cbaral/">Chitta Baral</a> at ASU, 
        and closely collaborate with 
        <a href="https://rushila.com">Rushil Anirudh</a>
        at
        <a href="https://computing.llnl.gov/">Lawrence Livermore National Laboratories</a>.
        I received my MS in ECE from 
        <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a>, 
        where I worked with 
        <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>. 
      </p>
      <p>
        <!-- Research Interest -->
        The main focus of my Ph.D. is on <b>robust visual understanding</b>, targetting problems such as domain generalization, robustness to logical, semantic, and linguistic shifts, robustness to common corruptions, geometric transformations, etc. 
        The solutions that I have developed towards this end fall under two main categories:
        <!-- <br> -->
        <!-- I work on computer vision, and in order to "understand" the visual world, work at the intersection of <b>vision and language</b>.  -->
        <!-- More specifically, I am interested in the following questions: -->
        <ul style="padding-left:20px;">
          <li> Knowledge-guided design of input augmentation functions </li>
          <li> Adversarially-guided discovery of input transformations </li> 
          <!-- <li> bridging Physical and Semantic Scene Understanding for better visual reasoning.</li> -->
        </ul>
        These ideas are quite relevant when it comes to generalizing machine learning models from synthetic data to real/lab data and in low-data settings where data augmentation plays a crucial role.
      </p>
      <hr/>
      <p>
        <table align="justify">
          <tr>
            <td> <h4><font size="4">AFFILIATIONS</font></h4> </td>
            <td align="center">
              <a href="https://www.llnl.gov/">
                <img src="images/llnl_logo.jpg" class="logo">
              </a>
            </td>
            <td align="center">  
              <a href="http://www.asu.edu">
                <img src="images/asu_logo.png" class="logo">
              </a>
            </td>
            <td align="center">
              <a href="http://www.cmu.edu">
                <img src="images/cmu_logo.png" class="logo">
              </a>
            </td>
            <td align="center">
              <a href="https://research.snap.com/">
                <img src="images/snap_logo.png" class="logo">
              </a>
            </td>
            <td align="center">
              <a href="https://www.bits-pilani.ac.in/">
                <img src="images/bits_logo.png" class="logo">
              </a>
            </td>
            <td align="center">
              <a href="http://hkn.ece.cmu.edu/">
                <img src="images/hkn_logo.png">
              </a>
            </td>
          </tr>
          <tr>
            <td></td>
            <td align="center"><b>Lawrence Livermore <br>National Laboratory</b><br />Summer 2020, 2021</td>
            <td align="center"><b>Arizona State <br>University</b><br />2018-present</td>
            <td align="center"><b>Carnegie Mellon <br>University</b><br />2016-2018</td>
            <td align="center"><b>Snap Inc. <br>Research</b><br />Summer 2018</td>
            <td align="center"><b>BITS <br>Pilani<br /></b>2011-2015</td>
            <td align="center"><b>IEEE <br>Eta Kappa Nu</b><br />2017-present</td>
          </tr>
        </table>
      </p>
      <hr/>
      <br>

      <h4><font size="4">SELECTED PUBLICATIONS</font></h4>
      
      <table style="border-collapse:separate; border-spacing:0 1em; width: 98%; margin-left: auto; margin-right: auto;">
        <tbody>
          <tr>
            <td class="teaser">
              <a href=""><img src="images/spatial_vqa.png" class="teaser"></a>
            </td>
            <td class="panel">
              <a href="https://arxiv.org/abs/2109.01934" class="papertitle">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering</a>
              <br/><venue>ICCV 2021</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <b>Tejas Gokhale</b>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
              <br>
              <a href="https://arxiv.org/pdf/2109.01934.pdf" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">
                VQA models trained with two additional objectives: object centroid estimation and relative position estimation, lead to improved performance on spatial reasoning questions (in GQA) in fully supervised and few shot settings as well as improved O.O.D. generalization.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/synth_data.png"><img src="images/synth_data.png" class="teaser"></a>
            </td>

            <td class="panel">
              <a href="https://arxiv.org/abs/2012.02356" class="papertitle">WeaQA: Weak Supervision via Captions for Visual Question Answering</a>
              <br/><venue>ACL 2021 Findings</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <b>Tejas Gokhale</b>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
              <br>
              <a href="https://arxiv.org/pdf/2012.02356.pdf" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">
                We show that models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions.  Our experiments suggest gains on benchmark with shifted priors (VQA-CP) over baselines which use full supervision from human-authored QA data.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/teaser_ext.png"><img src='images/teaser_ext.png' class="teaser"></a>
            </td>
            <td class="panel">
              <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
              <br/><venue class="arxiv">CVPR 2021 Workshop</venue>, "AI for Content Creation"
              <br>
              <a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
              <b>Tejas Gokhale</b>,
              <a href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
              <a href="https://pavanturaga.com/">Pavan Turaga</a>,
              <a href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
              <br>
              <a href="https://arxiv.org/pdf/2004.08614.pdf" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">             
                Scene completion from sparse and incomplete label maps.
                `Halluci-Net' is a 2-stage method that 
                captures the object co-occurrence relationships,
                to produce dense label maps from incomplete labelmaps and object boundaries,
                for image synthesis.
              </p>
              
            </td>
          </tr>
          <tr>
            <td class="teaser">
                <a href="images/ttl_rc.png"><img src="images/ttl_rc.png" class="teaser"></a>
            </td>

            <td class="panel">
              <a class="papertitle" href="https://arxiv.org/abs/2103.11263" >
                Self-Supervised Test-Time Learning for Reading Comprehension
              </a>
              <br/><venue>NAACL 2021</venue>
              <br>
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <b>Tejas Gokhale</b>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
              <br>
              <a href="https://arxiv.org/pdf/2103.11263.pdf" style="color:magenta;">[Paper]</a>

              <p class="tag" style="text-align:justify">
                Unsupervised Reading Comprehension method that operates directly on a single test passage.
                Synthetic QA pairs are generated from the passage, and models are trained on these.
                When a new human-authored test question appears, models infer answers better than previous unsupervised methods.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/agat_overall_new.png"><img src="images/agat_overall_new.png" class="teaser"></a>
            </td>

            <td class="panel">
              <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
                Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
              </a>
              <br/><venue>AAAI 2021</venue>
              <br>
              <b>Tejas Gokhale</b>,
              <a href="https://rushila.com/">Rushil Anirudh</a>,
              <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
              <a href="https://jjthiagarajan.com/">Jayaraman Thiagarajan</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
              <br>
              <a href="https://arxiv.org/pdf/2012.01806.pdf" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">
                An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
                Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations --- object-related shifts, geometric transformations, and common image corruptions.
              </p>
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/overall_final.png"><img src="images/overall_final.png" class="teaser"></a>
            </td>

            <td class="panel">
              <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
              </a>
              <br/><venue>EMNLP 2020</venue>
              <br>
              <b>Tejas Gokhale*</b>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <br>
              <a href="https://arxiv.org/abs/2009.08566" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">
                MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
                We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
                MUTANT establishes a new SOTA (+10%) on the VQA-CP challenge (for generalization under Changing Priors)
              </p>              
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/v2c_tgokhale.png"><img src='images/v2c_tgokhale.png' class="teaser"></a>
            </td>

            <td class="panel">
              <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
              </a>
              <br/><venue>EMNLP 2020</venue>
              <br>
              <a href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
              <b>Tejas Gokhale*</b>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <br>
              <a href="https://asu-active-perception-group.github.io/Video2Commonsense/" style="color:magenta;">[Website]</a>,
              <a href="https://arxiv.org/pdf/2003.05162.pdf" style="color:magenta;">[Paper]</a>
              <p class="tag" style="text-align:justify">
                Actions in videos are inherently linked to latent social and commonsense aspects.
                We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
                <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
                Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
              </p>      
            </td>
          </tr>

          <tr>
            <td class="teaser">
              <a href="images/lol_tgokhale.png"><img src='images/lol_tgokhale.png' class="teaser"></a>
            </td>
            <td class="panel">
              <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
              </a>
              <br><venue>ECCV 2020</venue>
              <br>
              <b>Tejas Gokhale*</b>,
              <a href="https://pratyay-banerjee.github.io/">Pratyay Banerjee*</a>,
              <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <br>
              <a href="./vqa_lol.html" style="color:magenta;">[Website]</a>, 
              <a href="https://arxiv.org/pdf/2002.08325.pdf" style="color:magenta;">[Paper]</a>, 
              <a href="https://youtu.be/u6dEKvwla9M" style="color:magenta;">[Video]</a>
              <p class="tag" style="text-align:justify">
                VQA models struggle at negation, antonyms, conjunction, disjunction!
                We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
              </p>
            </td>
          </tr>



          <tr>
            <td class="teaser">
              <a href="images/blocksworld_tgokhale.PNG"><img src='images/blocksworld_tgokhale.PNG' class="teaser"></a>
            </td>
            <td class="panel">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="papertitle">Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs
              </a>
              <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
              <br><b>Tejas Gokhale</b>,
              <a href="">Shailaja Sampat</a>,
              <a href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
              <a href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
              <br>
              <a href="https://asu-active-perception-group.github.io/bird_dataset_web/" style="color:magenta;">[Website]</a>, 
              <a href="https://arxiv.org/pdf/1905.12042.pdf" style="color:magenta;">[Longer Preprint]</a>, 
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" style="color:magenta;">[CVPR-VMC Paper]</a>
              <p class="tag" style="text-align:justify">
                Given two images (source, target) with different object configurations, 
                what is the sequence of steps to re-arrange source to match target?
                For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
                and exhibits inductive generalization.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
    
      <hr/>
      <hr style="clear: both;visibility: hidden;"/>
      <h4><font size="4">PEOPLE</font></h4>
      <p style="padding-bottom: 0px;padding-top: 0px;">
        <b>Collaborators:</b>
        <ul>
          <table width="100%" border="0" cellpadding="0" cellspacing="0">
            <tr>
              <td style="width:20%;"><li><b>(ASU)</b></li></td>
              <td>
                <a href="http://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>, 
                <a href="https://pratyay-banerjee.github.io">Pratyay Banerjee</a>,
              </td>
            </tr>
            <tr>
              <td><li><b>(CMU / Adobe)</b></li></td>
              <td><a href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>,</td>
            </tr>
            <tr>
              <td><li><b>(LLNL)</b></li></td>
              <td>
                <a href="https://rushila.com/">Rushil Anirudh</a>, 
                <a href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
                <a href="https://jjthiagarajan.com/">Jay Thiagarajan</a>
              </td>
            </tr>
          </table>
        </ul>
      </p>

      <p style="padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <b>Mentees:</b>
        <ul>
          <table width="100%" border="0" cellpadding="0" cellspacing="0">
            <tr>
              <td style="width:20%;vertical-align:top;"><li><b>(MS Research)</b></li></td>
              <td>
                Adela Shao (2021- ),&nbsp;
                Maitreya Patel (2021- ),&nbsp;
                Abhishek Chaudhary (2020-2021 AY) &rarr; Amazon,&nbsp;
                Arnav Chakravarthy (2020-2021 AY) &rarr; VMWare,&nbsp;
                <a href="https://www.linkedin.com/in/aadhavan-sadasivam/">Aadhavan Sadasivam</a> (2019-20 AY) &&rarr; PayPal,&nbsp;
                Shivakshit Patri (2019 Spring) &rarr; Amazon
              </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top;"><li><b><a href="https://furi.engineering.asu.edu/">(ASU FURI)</a></b></li></td>
              <td><a href="https://www.linkedin.com/in/mertay-dayanc/">Mertay Dayanc</a> (2020) </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top;"><li><b>(BS Capstone)</b></li></td>
              <td>
                <a href="https://www.linkedin.com/in/paulfbutler2016">Paul Butler</a> (2019-20 AY) &rarr; Microsoft, Jace Lord, Sagarika Pannase, Aashwin Ranjan, William Tith.
              </td>
            </tr>
          </table>
        </ul>
      </p>
      <hr/>
      <hr style="clear: both;visibility: hidden;"/>
        <!-- <b>Friends who "Science":</b>
        <ul>
          <li><b>(ASU)</b>                
            <a href="https://kowshikthopalli.github.io/">Kowshik Thopalli</a>,          
            <a href="https://www.public.asu.edu/~lcheng35/">Lu Cheng</a>, 
            <a href="https://www.linkedin.com/in/joshua-feinglass-b1ba23a2">Joshua Feinglass</a>,
            <a href="http://samrawal.com/">Sam Rawal</a>, 
            <a href="https://www.linkedin.com/in/john-janiczek/">John Janiczek</a>, 
            <a href="https://www.taeyeongchoi.com/">Taeyeong Choi</a>, 
            <a href="https://www.linkedin.com/in/man-luo-a7aa57178/">Man Luo</a>
          </li>
          <li><b>(CMU)</b> 
            <a href="https://users.ece.cmu.edu/~vsaragad/">Vishwanath Saragadam</a>, 
            <a href="https://www.linkedin.com/in/bhargavghanekar">Bhargav Ghanekar </a>, 
            <a href="https://rachel-sunrui.github.io/"> Rachel Sun </a>
          </li>
          <li><b>(BITS)</b>
            <a href="https://scholar.google.com/citations?user=WyVypxAAAAAJ">Ninad Kanetkar (NEU BioEng)</a>, 
            <a href="https://www.leibniz-inm.de/en/staff/m-sc-bhusari-shardul-2/">Shardul Bhusari (Leibniz Institute Saarbrucken)</a>,
            <a href="http://ishankhurz.mystrikingly.com/">Ishan Khurjekar (UF ECE)</a>, 
            <a href="">Aseem Pradhan (GMU BioEng)</a>
          </li>
        </ul> -->

      <h4><font size="4">SERVICE</font></h4>
      <p style="padding-bottom: 0px;padding-top: 0px;">
        <ul>
          <table width="100%" border="0" cellpadding="0" cellspacing="0">
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Reviewer:</b> </li></td>
              <td>
                <a href="https://iclr.cc/Conferences/2022"> ICLR 2022</a>, <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>, <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>
                <!-- </li> -->
                <!-- <li><i>NLP:</i> -->
                <a href="https://2021.emnlp.org/">EMNLP 2021</a>,
                <a href="https://2021.naacl.org/">NAACL 2021</a>,
                <!-- </li> -->
                <!-- <li><i>Robotics:</i> -->
                ICRA <a href="https://www.icra2019.org/">2019</a>, <a href="https://www.icra2020.org/">2020</a>, <a href="http://www.icra2021.org/">2021</a>, <a href="https://www.ieee-ras.org/publications/ra-l">IEEE RA-L</a>, 
                <!-- </li> -->
                <!-- <li><i>Vision:</i> -->
                <a href="http://wacv2022.thecvf.com/">WACV 2022</a>, 
                <a href="https://www.springer.com/journal/138">Springer MVAP</a> 
                <!-- </li> -->
              </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Organizer / Host:</b></li></td>
              <td>
                <a href="https://asu-active-perception-group.github.io/seminar/">Spring 2021 Seminar Series (Frontiers in Vision and Language)</a>,
                <a href="reading_group.html">Summer Vision Reading Group</a>
              </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Advisor</b>:</li> </td>
              <td>ASU ML Club</li></td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Volunteer:</b></li></td>
              <td>
                <a href="https://icml.cc/Conferences/2020">ICML 2020</a>,
                <a href="https://swrobotics.engineering.asu.edu/wp-content/uploads/2019/10/SWRS_Program_2019_final.pdf">SWRS 2019</a>
              </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top;"><li><b>Research Mentor:</b></li></td>
              <td> ASU FURI, CSE485 Capstone (Cognitive Vision, Vision&Language)</td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Teaching:</b></li></td>
              <td>
                ASU CSE310: Data Structures and Algorithms (Taught Recitations), <br>
                ASU CSE408: Multimedia Information Systems (TA), <br>
                ASU CSE110: Principles of Programming (Taught Labs), <br> 
                ASU CSE576: Natural Language Processing (Mentored Class Projects), <br>
                BITS CTE: Advanced Image Processing (co-Instructor).
              </td>
            </tr>
            <tr>
              <td style="width:20%;vertical-align:top"><li><b>Student Mentor:</b> </li></td>
              <td>
                Graduate Student Mentorship Program (GSMP), ASU, 2019-present.
                <br>
                Peer Mentorship Program (first installment), BITS Goa, 2014
              </td>
            </tr>
          </table>          
        </ul> 
      </p>  
      <hr/>         
      <hr style="clear: both;visibility: hidden;"/>
      
      <h4><font size="4">AWARDS</font></h4>
      
      <p style="padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <b>Scholarships / Fellowships:</b>
        <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
          <li> <a href="https://graduate.asu.edu/current-students/funding-opportunities/awards-and-fellowships/university-graduate-fellowships"> CIDSE Doctoral Fellowship</a> (ASU, Spring 2020, Spring 2021)</li>
          <li> Engineering Graduate Fellowship</a> (ASU, Spring 2020)</li>
          <li> <a href="http://www.ncert.nic.in/programmes/talent_exam/index_talent.html">National Talent Scholarship</a>, (Govt. of India, 2007-2015)</li>
        </ul>
      </p>
      <p style="padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <b>Travel Awards</b>
        <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
          <li> Graduate College Travel Award, ASU (for ICCV 2021, EMNLP 2020, ECCV 2020)</li>
          <li> IJCAI Doctoral Consortium Travel Award, (IJCAI, 2019)</li>
          <li> CIDSE Travel Grant Award, (for CVPR 2019) </li>
        </ul>
      </p>
      <p style="padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
        <b>Societies / Memberships</b>
        <ul style="margin-bottom: 0px;padding-bottom: 0px;margin-top: 0px;padding-top: 0px;">
          <li> Inducted, <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu Sigma Chapter</a> (Carnegie Mellon University, 2017)</li>
          <li> Member, <a href="https://www.thecvf.com/">Computer Vision Foundation</a>, <a href="https://www.aclweb.org">Association for Computational Linguistics</a>, <a href="https://aaai.org/">Association for the Advancement of Artificial Intelligence</a></li> 
        </ul> 
      </p> 
    </div> 


<!--     <div class="thumbnail", style="max-width:300px;">
      <h4><font size="4">NEWS</font></h4>
      <p class="tag">
        <b><font color="000000">May 2021:</font></b>
        Back as an intern at Lawrence Livermore National Laboratory, mentored by Rushil Anirudh, Jay Thiagarajan, and Bhavya Kailkhura
      </p>
      <p class="tag">
        <b><font color="000000">May 2021:</font></b>
        Restarted the <a href="./reading_group.html">Summer Vision Reading Group</a>
      </p>
      <p class="tag">
        <b><font color="000000">Mar 2021:</font></b>
        I was awarded the CIDSE Doctoral Fellowship for Spring 2021.</a>.
      </p>
      <p class="tag">
        <b><font color="000000">Jan 2021:</font></b>
        Co-organizing the <a href="https://asu-active-perception-group.github.io/seminar/">ASU-APG Seminar Series </a> on "Frontier Topics in Vision and/or Language" in Spring 2021.
      </p>

      <p class="tag">
        <b><font color="000000">Oct 2020:</font></b>
        Hosted the first "Season" of <a href="./reading_group.html"> Summer Vision Reading Group </a> at ASU. (May-Oct 2020)
      </p>

      <p class="tag">
        <b><font color="000000">May 2020:</font></b>
        Internship at Lawrence Livermore National Labs mentored by <a href="https://www.rushila.com">Rushil Anirudh</a>
      </p>

      <p class="tag">
        <b><font color="000000">Mar 2020:</font></b>
        I was awarded the CIDSE Doctoral Fellowship and the Engineering Graduate Fellowship.</a>.
      </p>

      <p class="tag">
        <b><font color="000000">Aug 2019:</font></b>
        "Vision beyond Pixels", <i><a href='./presentations/tgokhale_ijcai2019dc.pdf'> <font color="magenta">Talk</font> </a></i> at IJCAI 2019 Doctoral Consortium, <i><font color="ForestGreen">Received IJCAI DC Travel Award</font></i>
      </p>

      <p class="tag">
        <b><font color="000000">Jul 2019:</font></b>
        "Reasoning about Objects and Actions via Block-Play", <font color="magenta">Talk</font> at Telluride 2019 <a href="https://sites.google.com/view/telluride2019/home">Neuromorphic Cognition Engineering Workshop</a>
      </p>

      <p class="tag">
        <b><font color="000000">Aug 2018:</font></b>
        Joined <a href="https://yezhouyang.engineering.asu.edu/research-group/">ASU Active Perception Group </a> as a PhD student with Yezhou Yang. Co-advised by Chitta Baral.
      </p>

      <p class="tag">
        <b><font color="000000">May 2018:</font></b>
        Internship at Snap Research, Seattle with <a href="https://www.linkedin.com/in/krishnanguru/">Guru Krishnan</a> and <a href="http://www.cs.columbia.edu/~nayar/">Shree Nayar</a>
      </p>

      <p class="tag">
        <b><font color="000000">Apr 2018:</font></b>
        "Deep Learning Methods in Imaging and Computer Vision", <i><a href="https://touch.facebook.com/AlumniRelationsBITSGoa/photos/a.577537568978007/1797680853630333/?type=3&source=54"> <font color="magenta">Invited Talk</font></a></i> at BITS Pilani.
      </p>

      <p class="tag">
        <b><font color="000000">Dec 2017:</font></b>
        Graduated with M.S. (ECE) from <a href="https://www.ece.cmu.edu/">Carnegie Mellon University</a> (with <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu </a> Honors) 
      </p>

      <p class="tag">
        <b><font color="000000">Jan 2017:</font></b>
        Joined <a href="http://imagesci.ece.cmu.edu/">Image Science Lab</a> at CMU as a Graduate Researcher with Aswin Sankaranarayanan.
      </p>

      <h4>&nbsp;</h4>


      <h4><font size="4">AWARDS and HONORS</font></h4>
      <h4><font size="2">Scholarships / Fellowships</font></h4>
      <ul class="tag">
        <li> <a href="http://www.ncert.nic.in/programmes/talent_exam/index_talent.html">National Talent Scholarship</a>, (Govt. of India, 2007-2015)</li>
        <li> Engineering Graduate Fellowship</a> (ASU Engineering, Spring 2020)</li>
        <li> <a href="https://graduate.asu.edu/current-students/funding-opportunities/awards-and-fellowships/university-graduate-fellowships"> CIDSE Doctoral Fellowship</a> (CIDSE, ASU, Spring 2020, Spring 2021)</li>
      </ul>

      <h4><font size="2">Travel Awards</font></h4>
      <ul class="tag">
        <li>Graduate College Travel Award, ASU (for EMNLP 2020)</li>
        <li>Graduate College Travel Award, ASU (for ECCV 2020)</li>
        <li> IJCAI Doctoral Consortium Travel Award, (IJCAI, 2019)</li>
        <li> CIDSE Travel Grant Award, (for CVPR 2019) </li>
      </ul>

      <h4><font size="2">Societies / Memberships</font></h4>
      <ul class="tag">
        <li> Inducted, <a href="http://hkn.ece.cmu.edu/">Eta Kappa Nu Sigma Chapter</a> (Carnegie Mellon University, 2017)</li>
        <li> Member, <a href="https://www.thecvf.com/">Computer Vision Foundation</a>, <a href="https://www.aclweb.org">Association for Computational Linguistics</a>, <a href="https://aaai.org/">Association for the Advancement of Artificial Intelligence</a></li> 
      </ul>           
    </div> -->
  </div>
</body>
</html>