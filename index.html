<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tejas Gokhale</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">    
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,600,300" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="theme.css">

    <link href="font-awesome-4.7.0/css/font-awesome.css" rel="stylesheet" type="text/css">
    <link href="academicons-1.9.4/css/academicons.css" rel="stylesheet" />
    <link rel="shortcut icon" type="image/png" href="images/tejas/tg_hawaii_square.png"/>
    
  </head>
<body>
  
<div class="container" style="background-color: #fcfcfc;">
  <div class="row mb-3 justify-content-center">
    <div class="col-md-4" style="font-family: Lato, Helvetica, arial, sans-serif;">
      <h1 style="font-weight:500;text-transform: capitalize;">Tejas <b>Gokhale</b></h1>
    </div>
    <div class="col-md-8">
      <div class="topnav">
        <a href="./seminar.html">SEMINAR</a>
        <a href="./index.html#teaching">TEACHING</a>
        <a href="./index.html#people">PEOPLE</a>
        <a href="./index.html#publications">RESEARCH</a>
        <a href="./index.html">HOME</a>
      </div>
    </div>


    <div class="col-md-4">
      <br/>
      <!-- <img src="images/tejas/tg_photo_1.jpg" alt="Tejas Gokhale" class="portrait" style="max-width: 75%;"> -->
      <div>
        <img class="portrait" id="randomImage" alt="Tejas Gokhale" style="max-width: 75%;"/>
      </div>
      <script>
        index = Math.floor(Math.random() * 2);
        document.getElementById("randomImage").src = "images/tejas/tg_photo_" + index + ".jpg";
      </script>
      <!-- <script>
        const images = [
            'images/tejas/tg_photo_0.jpg',
            'images/tejas/tg_photo_1.jpg'
        ];

        function getRandomImage() {
            const randomIndex = Math.floor(Math.random() * images.length);
            return images[randomIndex];
        }

        document.addEventListener('DOMContentLoaded', () => {
            const randomImage = getRandomImage();
            document.getElementById('randomImage').src = randomImage;
        });
    </script> -->
      <br/>
      <br/>
      <p style="font-weight: 350;">
        <font style="font-weight: 700;">Assistant Professor</font>
        <br/><a href="https://www.csee.umbc.edu/">Computer Science & Electrical Engineering</a>
        <br/><a href="https://umbc.edu/">University of Maryland, Baltimore County</a>
      </p>

      <p style="font-weight: 350;">
        <font style="font-weight: 700;">Director</font>
        <br/><a href="">Cognitive Vision Group</a>
        <div>
          <img class="portrait" src="images/logos/cvg_square.PNG" alt="Cognitive Vision Group" style="max-width: 40%;"/>
        </div>
      </p>

      <p style="font-weight: 350;">
        <font style="font-weight: 700;">Affiliate Faculty</font>
        <br/><a href="https://ai.umbc.edu/">UMBC Center for AI</a>
      </p>
      <p> 
        <a href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ"><i class="ai ai-google-scholar-square ai-2x"></i></a>
        <a href="https://dblp.org/pid/241/9472.html"><i class="ai ai-dblp-square ai-2x"></i></a>
        <a href="https://www.semanticscholar.org/author/Tejas-Gokhale/120838645"><i class="ai ai-semantic-scholar-square ai-2x"></i></a>
        <a href="tgokhale_resume.pdf"><i class="ai ai-cv-square ai-2x"></i></a>
        <a href="https://www.linkedin.com/in/tejas-gokhale-78488a1a8/"><i class="fa fa-linkedin-square fa-2x-linkedin"></i></a>
      </p>
      <p>
        <ul style="padding-left: 12pt;">
          <li><a href="seminar.html"><font style="font-weight:800;color:#AA0000;">PPR Seminar</font></a></li>
        <li><a href="https://link.springer.com/book/10.1007/978-3-031-57816-8"><font style="font-weight:800;color:#AA0000;">Book on Multimodal Retrieval & Generation</font></a></li>
      </ul>
      </p>
      <p id="teaching" style="font-weight: 350;">
        <big><font style="font-weight: 700;">Teaching</font></big><br/>
        CMSC 475/675 Neural Networks [<a href="https://courses.cs.umbc.edu/graduate/675/">S25</a>]
        <br>
        CMSC 491/691 Robust Machine Learning [<a href="https://courses.cs.umbc.edu/graduate/691rml/">F24</a>]
        <br>
        CMSC 491/691 Computer Vision [<a href="https://courses.cs.umbc.edu/graduate/691cv/">S24</a>, F23]
      </p>
      <!-- <p style="font-weight: 350;"> -->
        <!-- <big><font style="font-weight: 700;">Useful Links</font></big> -->
        <!-- <br><a href="https://www.csee.umbc.edu/"> UMBC CSEE</a> -->
        <!-- <br><a href="https://ai.umbc.edu/"> UMBC Center for AI </a> -->
        <!-- <br><a href="https://meyerhoff.umbc.edu/"> Meyerhoff Scholars</a> -->
        <!-- <br><a href="https://cwit.umbc.edu/cwitscholars/"> Center for Women in Technology</a> -->
        <!-- <br><a href="https://ur.umbc.edu/"> Undergraduate Research Opportunities</a> -->
        <!-- <br><a href="https://umbc.edu/about/"> UMBC Stats & Rankings </a> -->
      <!-- </p> -->
      <p id="officehours" style="font-weight: 350;">
        <big><font style="font-weight: 700;">Office Hours</font></big><br/>
        W 1430--1530; ITE 214
      </p>
      <!-- <p id="travel" style="font-weight: 350;"> -->
        <!-- <big><font style="font-weight: 700;">Upcoming Travel</font></big><br/> -->
        <!-- Oct 25-27, <a href="https://aaai.org/conference/fall-symposia/aaai-2023-fall-symposium-series/">AAAI Fall Symposium 2023 </a> <br/> -->
        <!-- Nov 06-09, <a href="https://www.corl2023.org/">CoRL 2023 </a> <br/> -->
        <!-- Dec 11-16, <a href="https://nips.cc/Conferences/2023/Dates"> NeurIPS 2023 </a> <br/> -->
        <!-- Feb 21-26, <a href="https://aaai.org/aaai-conference/"> AAAI 2024 </a> -->
        <!-- Jun 17-21, <a href="https://cvpr.thecvf.com/"> CVPR 2024 </a> -->
      <!-- </p> -->
<!--       <div class="col-md-4 justify-content-center">
        <img src="images/logos/rpg_logo.bmp" width="200px">
      </div> -->
    </div>

    <div class="col-md-8" style="padding-right:32px;font-weight: 350;">
      <br/>
      <p>
        I am a computer vision researcher; my research draws inspiration from principles of perception, communication, learning, and reasoning.  
        I am the Director of the Cognitive Vision Group at UMBC, where we broadly work on conceptual characterization of visual scenes, with some goals including interpretation of visual data in presence of incomplete information, recognizing and adapting to novelty and variations, leveraging external knowledge and reasoning modules to generalize to new contexts, domains, environments, and tasks, acquiring visual knowledge and communicating it to other machines and humans.  
        I received my Ph.D. from Arizona State University, my M.S. from Carnegie Mellon University, and my B.E. (Honours) degree from Birla Institute of Technology and Science, Pilani.
      </p>
      <!-- <p>
        I received my Ph.D. from Arizona State University where I was advised by <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> and <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>, M.S. from Carnegie Mellon University where I worked with <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>, and B.E. (Honours) from Birla Institute of Technology and Science. During my graduate studies I worked with wonderful collaborators at Lawrence Livermore National Laboratory, Microsoft Research, and Snap Research.
      </p> -->

      <p>I run the <a href="./seminar.html">Perception, Prediction, and Reasoning Seminar</a> at UMBC.</p>

      <div>
        <div class="col-md-12" style="border-radius: 20px; background-color: #f5e1e1;padding-top:16px;padding-bottom:16px;padding-left:32px;padding-right:32px;">  
          <p style="font-weight:normal; margin:0px; color: black">
            <!-- <i class="fa fa-bullhorn"></i>  -->
            <!-- <b>Join the Group!</b> -->
            If you're interested in working with the Cognitive Vision Group 
            please read <b><a style="color: #ba5353" href="./docs/cvg_starter_pack.pdf">this note</a></b> 
            and use this <b><a style="color: #ba5353" href="https://forms.gle/dtGHQYY8o4nL2UHt6">form</a></b> to indicate interest.
            This <b><a style="color: #ba5353" href="./faq.html">FAQ</a></b> page might also be useful.
          </p>
        </div>
        <br/>
        <div class="col-md-12">
          <p>
            <center><h5> News </h5></center>
            <table class="color-table">
              <colgroup>
                <col style="width:25%">
                <col style="width:75%">
              </colgroup>
              <tbody>
                <tr>
                  <td>JAN 2025</td>
                  <td>Received funding from UMBC <a href="https://cybersecurity.umbc.edu/">Cybersecurity Institute</a> </td>
                </tr>
                <tr>
                  <td>DEC 2024</td>
                  <td>Co-PI on a $3.8M grant from DARPA for the <a href="https://www.darpa.mil/program/scientific-feasibility">SciFy</a> program.</td>
                </tr>
                <tr>
                  <td>SEP 2024</td>
                  <td><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/aaai.12194">AI Magazine Article</a> summarizing our work so far.</td>
                </tr>
                <tr>
                  <td>SEP 2024</td>
                  <td>Tutorial at ECCV 2024 on <a href="https://asu-apg.github.io/rbgm/">Responsibly Building Generative Models</a> </td>
                </tr>
                <tr>
                  <td>JUL 2024</td>
                  <td>New <a href="https://link.springer.com/book/10.1007/978-3-031-57816-8">Book</a> on Multimodal Retrieval and Generation</td>
                </tr>
                <tr>
                  <td>JUN 2024</td>
                  <td>Our paper led by Yiran Luo wins <a href="images/cvpr2024_vdu_best_paper_certificate.pdf">Best Paper Award</a> at <a href="https://sites.google.com/view/vdu-cvpr24/">VDU Workshop</a> @ CVPR 2024 </td>
                </tr>
                <tr>
                  <td>JUL 2024</td>
                  <td>Received START funding from UMBC <a href="https://research.umbc.edu/">ORCA</a></td>
                </tr>
                <tr>
                  <td>JUN 2024</td>
                  <td>Performed for the CVPR House Band; Seattle Convention Center</a></td>
                </tr>
                <tr>
                  <td>JUN 2024</td>
                  <td>Participating in <a href="https://hltcoe.jhu.edu/research/scale/scale-2024/">SCALE 2024</a> (Video-Based Event Retrieval) at <a href="https://hltcoe.jhu.edu/">JHU HLTCOE</a></td>
                </tr>
                <tr>
                  <td>MAY 2024</td>
                  <td>Received SURFF funding from UMBC <a href="https://research.umbc.edu/">ORCA</a></td>
                </tr>
                <tr>
                  <td>APR 2024</td>
                  <td>Serving as Area Chair for Neurips, ACL, NAACL</a> </td>
                </tr>
                <tr>
                  <td>MAR 2024</td>
                  <td>Received in-kind support from Microsoft Research under the <a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/projects/">Accelerate Foundation Models Academic Research Initiative</a> </td>
                </tr>
                <tr>
                  <td>FEB 2024</td>
                  <td>Invited Talk at AAAI 2024 <a href="https://aaai.org/aaai-conference/aaai-24-new-faculty-highlights/">New Faculty Highlights</a> </td>
                </tr>
                <tr>
                  <td>FEB 2024</td>
                  <td>Lightning Talk at IARPA <a href="https://www.iarpa.gov/research-programs/video-lincs">Video-LINCS</a> Proposers Day </td>
                </tr>
                <tr>
                  <td>JAN 2024</td>
                  <td>Tutorial at WACV 2024 on <a href="https://asu-apg.github.io/rgmv/">Reliability of Generative Models in Vision</a> 
                    <br><small>Talk: "Challenges with Evaluation of Text-to-Image Generation"</small></td>
                </tr>
<!--                 <tr>
                  <td>NOV 2023</td>
                  <td>Started the <a href="seminar.html">PPR Seminar</a> at UMBC</td>
                </tr>
                <tr>
                  <td>NOV 2023</td>
                  <td><a href="https://prg.cs.umd.edu/fall2023talk2.html">Invited Talk</a> at UMD PRG Seminar Series</td>
                </tr>
                <tr>
                  <td>SEP 2023</td>
                  <td>Joined <a href="https://cwit.umbc.edu/cwitscholars/">CWIT</a> as a mentor</td>
                <tr>
                  <td>AUG 2023</td>
                  <td>Moved to Maryland after 5 years of "it's a dry heat"</td>
                </tr> -->
                <!-- <tr>
                  <td>JUN 2023</td>
                  <td>Organized <a href="https://asu-apg.github.io/odrum/" >O-DRUM 2023</a> at CVPR 
                    <br/><small>(Workshop on Open-Domain Reasoning Under Multi-Modal Settings)</small>
                  </td>
                </tr> -->
                <!-- <tr>
                  <td>Apr 2023</td>
                  <td>
                    Defended my Ph.D !!!
                    <br/><small>Awarded the ASU Engineering Graduate Fellowship, ASU SCAI Doctoral Fellowship, GPSA Outstanding Research Award, and GPSA Outstanding Mentor Award.</small>
                  </td>
                </tr> -->
                <!-- <tr>
                  <td>Feb-Apr 2023</td>
                  <td>Invited Talks on "Reliable Semantic Vision" at 
                    <i><small>
                      <ul>
                        <li>Rochester Institute of Technology</li>
                        <li>SUNY Binghamton</li>
                        <li>Indiana University</li>
                        <li>University of Maryland Baltimore County</li>
                        <li>Case Western Reserve University</li>
                        <li>Colorado School of Mines</li>
                        <li>Temple University</li>
                      </ul>
                    </small></i>
                  </td>
                </tr>
                <tr>
                  <td>Jan 2023</td>
                  <td>Organized <a href="https://asu-apg.github.io/serum/">SERUM</a> Tutorial at WACV 2023 
                    <br><small>(Tutorial on Semantic Data Engineering under Multimodal Settings) </small></td>
                </tr> -->
                <!-- <tr>
                  <td>Oct 2022</td>
                  <td>Recognized as <a href="https://neurips.cc/Conferences/2022/ProgramCommittee">Top Reviewer for NeurIPS 2022 </a></td>
                </tr>
                <tr>
                  <td>Jun 2022</td>
                  <td>Organized <a href="https://asu-apg.github.io/odrum/archive_2022.html" >O-DRUM 2022</a> (1st Workshop on Open-Domain Retrieval Under Multi-Modal Settings) at CVPR </td>
                </tr> -->
              </tbody>
            </table>
          </p>
        </div>
      </div>
    </div>
  </div>


  <hr />


  <div class="row col-md-12 justify-content-left">
    <div class="col-md-12 mb-3" style="font-size:14px;">
      <a><h3 id="books">Books</h3></a>
      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/book_springer_mmirg.webp" ><img align="left" class="figure-img" src="images/book_springer_mmirg.webp" style="max-height: 240px;"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://link.springer.com/book/10.1007/978-3-031-57816-8" class="papertitle">Advances in Multimodal Information Retrieval and Generation</a>
          <br/><venue>Springer</venue>
          <br>
          <a class="author" href="https://luomancs.github.io/">Man Luo</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          
          <br>
          <a href="https://spright-t2i.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <p class="tag">
            A comprehensive overview of the state-of-the-art methods in multimodal retrieval, generation, and retrieval-augmented generation.
            <br><br>
            <i> Series Title: <a href="https://www.springer.com/series/16902/books">Synthesis Lectures on Computer Vision</a></i>
            <br><br>
            <i>Hardcover ISBN: 978-3-031-57816-8; Softcover ISBN978-3-031-57818-2; eBook ISBN: 978-3-031-57816-8</i>
        </div>
      </div>

      <a><h3 id="publications">Publications</h3></a>
      <!-- <p style="font-weight: 350;"> -->
        <!-- Also available via <a href="https://scholar.google.com/citations?user=_ILTlEwAAAAJ">Google Scholar</a>. -->
        <!-- As a matter of principle, I only publish in (and review for) open-access conferences and journals. -->
      <!-- </p> -->
      <!-- <small>* indicates equal contribution</small><br /> -->


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/voila_teaser.png" ><img class="figure-img" src="images/teasers/voila_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="" class="papertitle">Voila: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning</a>
          <br/><venue> ICLR 2025 </venue>
          <br>
          <a class="author" href="https://www.linkedin.com/in/nilay-yilmaz">Nilay Yilmaz</a>, 
          <a class="author" href="https://maitreyapatel.com/">Maitreya Patel</a>, 
          <a class="author" href="https://scholar.google.com/citations?user=n4IrxbUAAAAJ&hl=en">Yiran Luo</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://sites.google.com/asu.edu/imaging-lyceum">Suren Jayasuriya</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <!-- <a href="https://arxiv.org/pdf/2411.02545" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a> -->
          <!-- <a href="https://tripletclip.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a> -->
          <!-- <a href="https://github.com/tripletclip/TripletCLIP" class="badge badge-code badge-sm text-decoration-none mb-1">code</a> -->
          <!-- <a href="https://huggingface.co/TripletCLIP" class="badge badge-data badge-sm text-decoration-none">data</a> -->
          <p class="tag">
            VOILA, a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMs’ perceptual understanding and abstract relational reasoning. VOILA employs an analogical mapping approach in the visual domain, requiring models to generate an image that completes an analogy between two given image pairs, reference and application, without relying on predefined choices.
          </p>
        </div>
      </div>


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/tips_teaser.png" ><img class="figure-img" src="images/teasers/tips_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2404.07410" class="papertitle">Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling</a>
          <br/><venue> WACV 2025 </venue>
          <br>
          <a class="author" href="https://sourajitcs.github.io/">Sourajit Saha</a>, 
          <b><a class="author">Tejas Gokhale</a></b>
          <br>
          <a href="https://arxiv.org/pdf/2404.07410.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/sourajitcs/tips/" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <p class="tag">
            We identify that the tendency of existing pooling layers in CNNs to pass larger signals to subsequent layers is a major factor that's strongly correlated with the lack of shift invariance in CNNs.
            Based on this finding, we design a new pooling operator Translation-Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps to learn translation-invariant representations.
            TIPS results in consistent and architecture-agnostic improvements in accuracy and four measures of shift invariance, across multiple image classification and segmentation benchmarks.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/tripletclip_teaser.png" ><img class="figure-img" src="images/teasers/tripletclip_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2408.02231" class="papertitle">TripletCLIP: Improving Compositional Reasoning of CLIP via Vision-Language Negatives</a>
          <br/><venue> NeurIPS 2024 </venue>
          <br>
          <a class="author" href="https://maitreyapatel.com/">Maitreya Patel</a>, 
          <a class="author" href="https://www.linkedin.com/in/abhiram-kusumba-a077b8174/">Abhiram Kusumba</a>, 
          <a class="author" href="https://shengcheng.github.io/">Sheng Cheng</a>, 
          <a class="author" href="https://www.changhoonkim.com/">Changhoon Kim</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2411.02545" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://tripletclip.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/tripletclip/TripletCLIP" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://huggingface.co/TripletCLIP" class="badge badge-data badge-sm text-decoration-none">data</a>
          <p class="tag">
            A method for generating ``hard'' negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators.
            A novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. 
            Our method "TripletCLIP" enhances the compositional capabilities of CLIP as well as improvements in zero-shot image classification and image retrieval.
          </p>
        </div>
      </div>
      
      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/spright_teaser.png" ><img class="figure-img" src="images/teasers/spright_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2404.01197" class="papertitle">Getting it Right: Improving Spatial Consistency in Text-to-Image Models</a>
          <br/><venue> ECCV 2024 </venue>
          <br>
          <a class="author" href="https://agneetchatterjee.com/">Agneet Chatterjee</a>, 
          <a class="author" href="https://www.linkedin.com/in/gabriela-ben-melech/">Gabriela Ben Melech Stan</a>, 
          <a class="author" href="https://www.linkedin.com/in/estelle-aflalo/">Estelle Aflalo</a>, 
          <a class="author" href="https://sayak.dev/">Sayak Paul</a>, 
          <a class="author" href="https://djghosh13.github.io/">Dhruba Ghosh</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt, 
          <a class="author" href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a>, 
          <a class="author" href="https://www.linkedin.com/in/vasudev-lal-79bb336/">Vasudev Lal</a>, 
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2404.01197.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://spright-t2i.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/SPRIGHT-T2I/SPRIGHT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://huggingface.co/spaces/SPRIGHT-T2I/SPRIGHT-T2I" class="badge badge-video badge-sm text-decoration-none">demo</a>
          <p class="tag">
            We improve spatial understanding of T2I models by creating the SPRIGHT dataset by recaptioning 6M images from widely used vision datasets.
            Finetuning T2I models with just 500 images from SPRIGHT leads to a large improvement in T2I spatial understanding performance, across several evaluation benchmarks such as T2I-CompBench, VISOR, and GenEval.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/revision_teaser.png" ><img class="figure-img" src="images/teasers/revision_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2408.02231" class="papertitle">REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models</a>
          <br/><venue> ECCV 2024 </venue>
          <br>
          <a class="author" href="https://agneetchatterjee.com/">Agneet Chatterjee</a>, 
          <a class="author" href="https://scholar.google.com/citations?user=n4IrxbUAAAAJ&hl=en">Yiran Luo</a>,  
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2408.02231.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://agneetchatterjee.com/revision/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/agneet42/revision" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://huggingface.co/revision-t2i" class="badge badge-data badge-sm text-decoration-none">data</a>
          <p class="tag">
            Traditional generative Text-to-Image models struggle to generate images that faithfully represent the spatial relationships mentioned in the input prompt. 
            We develop REVISION, an efficient rendering pipeline that enables a training-free, guidance-based mechanism to address this shortcoming. 
            REVISION takes the objects and their spatial relationships parsed from the given input prompt and synthesizes an image in Blender, placing the respective object assets at coordinates corresponding to the parsed spatial relationship.
            Given a user-provided input prompt T, we synthesize an image using REVISION and use it to guide existing T2I pipelines such as Stable Diffusion or ControlNet to obtain a spatially accurate output.
          </p>
        </div>
      </div>


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/langdepth_teaser.png" ><img class="figure-img" src="images/teasers/langdepth_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2404.08540" class="papertitle">On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation</a>
          <br/><venue> CVPR 2024 </venue>
          <br>
          <a class="author" href="https://agneetchatterjee.com/">Agneet Chatterjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2404.08540.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://agneetchatterjee.com/robustness_depth_lang/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/agneet42/robustness_depth_lang" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <p class="tag">
            The motivation of this work is to analyze the efficacy of language guidance for low-level non-semantic computer vision tasks. We focus on depth estimation and find that language-guided depth estimators benefit only from scene-level language information and counter-intuitively, are worse off when presented with sentences that describe 3D spatial relationships in the scene. With an increasing number of methods using language for depth estimation, our findings highlight the opportunities and pitfalls that require careful consideration for effective deployment in real-world settings. 
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/supermario_teaser.png" ><img class="figure-img" src="images/teasers/supermario_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2405.15961" class="papertitle">Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images</a>
          <br/><venue>CVPR 2024 Workshop on Vision Dataset Understanding</venue>
          <br>
          <a class="author" href="https://scholar.google.com/citations?user=n4IrxbUAAAAJ&hl=en">Yiran Luo</a>, 
          <a class="author" href="https://www.joshuafeinglass.com/">Joshua Feinglass</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="">Kuan-Cheng Lee</a>, 
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2405.15961" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/fpsluozi/SMD-SMOS" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://github.com/fpsluozi/SMD-SMOS" class="badge badge-data badge-sm text-decoration-none mb-1">data</a>
          <a href="images/cvpr2024_vdu_best_paper_certificate.pdf" class="badge badge-video badge-sm text-decoration-none"> [Best Paper Award] </a>
          <p class="tag">
            Two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. 
            New dataset: SuperMarioDomains (SMD) incorporating unique features of consistent classes of video game scenes across stylistic domains in video game graphics that are dissimilar to ImageNet1K.
          </p>
        </div>
      </div>
      
      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/conceptbed_teaser.png" ><img class="figure-img" src="images/teasers/conceptbed_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2306.04695" class="papertitle">ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models</a>
          <br/><venue> AAAI 2024 </venue> and <venue> Neurips 2023 Workshop on Diffusion Models</venue>
          <br>
          <a class="author" href="https://maitreyapatel.com/">Maitreya Patel</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2306.04695.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://conceptbed.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/ConceptBed/evaluations" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <p class="tag">
            Textual inversion models have the potential to learn novel concepts from a small number of example images.
            We quantify this concept learning ability with ConceptBed: a dataset that contains 284 unique visual concepts and 33K concept compositions, and 
            CCD (Concept Confidence Deviation): an evaluation metric uses the confidence of oracle concept classifiers to measure the alignment between generated images and concepts contained in ground truth images.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/aba_teaser.PNG" ><img class="figure-img" src="images/teasers/aba_teaser.PNG" class="figure-img"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2307.09520" class="papertitle">Adversarial Bayesian Augmentation for Single-Source Domain Generalization</a>
          <br/><venue>ICCV 2023</venue>
          <br>
          <a class="author" href="https://shengcheng.github.io/">Sheng Cheng</a>,  
          <b><a class="author">Tejas Gokhale</a></b>, 
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br/>
          <a href="https://arxiv.org/pdf/2307.09520.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/shengcheng/ABA" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <br/>
          <p class="tag">
            ABA draws on the strengths of adversarial learning and Bayesian neural networks to guide the generation of diverse data augmentations &mdash; these synthesized image domains aid the classifier in generalizing to several types of domain shift including style shift, subpopulation shift, and domain shift in the medical imaging setting.
            ABA outperforms all previous state-of-the-art methods, including pre-specified augmentations, pixel-based and convolutional-based augmentations.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/thesis_teaser.png" ><img class="figure-img" src="images/teasers/thesis_teaser.png" class="figure-img"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://keep.lib.asu.edu/items/187328" class="papertitle">Towards Reliable Semantic Vision</a>
          <br/><venue>Ph.D. Dissertation</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b> 
          <br/>
          <a href="https://keep.lib.asu.edu/system/files/c7/Gokhale_asu_0010E_22928.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <br/>
          <p class="tag">
            This dissertation contributes to the reliability of machine learning models from several perspectives including
            the development of robust training algorithms to mitigate the risks of such failures, construction of new datasets that provide a new perspective on capabilities of vision models, and the design of evaluation metrics for re-calibrating the perception of
            performance improvements.
            </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/reviz_teaser.PNG" ><img class="figure-img" src="images/teasers/reviz_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2306.00424" class="papertitle">End-to-end Knowledge Retrieval with Multi-modal Queries</a>
          <br/><venue>ACL 2023</venue>
          <br>
          <a class="author" href="https://luomancs.github.io/">Man Luo</a>, 
          <a class="author" href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang</a>,
          <b><a class="author">Tejas Gokhale</a></b>, 
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br/>
          <a href="https://arxiv.org/pdf/2306.00424.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/luomancs/remuq" class="badge badge-data badge-sm text-decoration-none mb-1">data</a>
          <br/>
          <p class="tag">
            Knowledge retrieval with multi-modal queries, i.e., queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval.
            A new dataset called ReMuQ, a new pretraining task for learning knowledge retrieval with multimodal queries, and a retriever model "ReViz" that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators.
          </p>
          
        </div>
      </div> 


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/mole_teaser.PNG" ><img class="figure-img" src="images/teasers/mole_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2303.17080" class="papertitle">Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling</a>
          <br/><venue>preprint</venue>
          <br>
          <a class="author">Ethan Wisdom</a>, 
          <b><a class="author">Tejas Gokhale</a></b>, 
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br/>
          <a href="https://arxiv.org/pdf/2303.17080.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/wisdeth14/MoleRecruitment" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <br/>
          <p class="tag">
            A data poisoning attack that confounds ML models without any manipulation of the image or label, achieved by simply leveraging the most confounding natural samples found within the training data itself.
            We show the efficacy of this novel attack in offline as well as continual learning (CL) settings in image classification, thereby exposing a previously undetected vulnerability of image classifiers.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/visor_teaser.png" ><img class="figure-img" src="images/teasers/visor_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2212.10015" class="papertitle">Benchmarking Spatial Relationships in Text-to-Image Generation</a>
          <br/><venue>preprint</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>, 
          <a class="author" href="https://hamidpalangi.com">Hamid Palangi</a>,
          <a class="author" href="https://besmiranushi.com">Besmira Nushi</a>,
          <a class="author" href="https://vibhav-vineet.github.io/">Vibhav Vineet</a>,
          <a class="author" href="https://erichorvitz.com/">Eric Horvitz</a>,
          <a class="author" href="https://ecekamar.com">Ece Kamar</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2212.10015.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://visort2i.github.io/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/microsoft/VISOR" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://www.microsoft.com/en-us/research/blog/frontiers-of-multimodal-learning-a-responsible-ai-approach/" class="badge badge-video badge-sm text-decoration-none"> [featured on MSR blog] </a>
          <p class="tag">
             We report a surprising finding that, although recent state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations such as left/right/above/below.
             We introduce a metric called VISOR to quantify spatial reasoning performance. VISOR can be used off-the-shelf with any text-to-image model.
             We construct and make available SR2D, a dataset which contains sentences that describe spatial relationships (left/right/above/below) between a pair of commonly occurring objects.
          </p>
        </div>
      </div>


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/alt_teaser.png" ><img class="figure-img" src="images/teasers/alt_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2206.07736" class="papertitle">Improving Diversity with Adversarially Learned Transformations for Domain Generalization</a>
          <br/><venue> WACV 2023</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://rushila.com/">Rushil Anirudh</a>,
          <a class="author" href="https://jjthiagarajan.com/">Jayaraman J. Thiagarajan</a>,
          <a class="author" href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2206.07736.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/ALT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://www.youtube.com/watch?v=3EktDD9JDNE" class="badge badge-video badge-sm text-decoration-none">video</a>

          <p class="tag">
            ALT discovers diverse and adversarial transformations using an image-to-image neural network with learnable weights.
            ALT improves the state-of-the-art single domain generalization performance on three benchmarks and is significantly better than pixel-wise adversarial training and standard data augmentation techniques.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/cripp_teaser.PNG" ><img class="figure-img" src="images/teasers/cripp_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2211.03779" class="papertitle">CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</a>
          <br/><venue> EMNLP 2022</venue>
          <br>
          <a class="author" href="https://maitreyapatel.com/">Maitreya Patel</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2211.03779.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://maitreyapatel.com/CRIPP-VQA/" class="badge badge-web badge-sm text-decoration-none mb-1">web</a>
          <a href="https://github.com/Maitreyapatel/CRIPP-VQA" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <p class="tag">
            Although the imaging pipeline is unable to capture many physical properties of objects (eg. mass and coefficient of friction), these properties can be estimated by utilizing cues introduced by collisions. 
            We introduce a new dataset (CRIPP-VQA) for reasoning about the implicit physical properties of objects from videos.
            The dataset contains videos of objects in motion, annotated with hypothetical/counterfactual questions about the effect of actions (removing/adding/replacing objects) and questions about planning (performing actions to reach a goal).            
          </p>
        </div>
      </div>


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <center><a href="images/teasers/interpolate_teaser.png" ><img class="figure-img" src="images/teasers/interpolate_teaser.png"></a></center>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://openreview.net/pdf?id=YkPjTHZDdm" class="papertitle">Covariate Shift Detection via Domain Interpolation Sensitivity</a>
          <br/><venue>NeurIPS 2022 Workshop on Interpolation and Beyond</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.joshuafeinglass.com/">Joshua Feinglass</a>, 
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://openreview.net/pdf?id=YkPjTHZDdm" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://youtu.be/75TvZMpDcvc" class="badge badge-video badge-sm text-decoration-none mb-1">video</a>
          <p class="tag">
            In this paper, we introduce a benchmark for covariate shift detection (CSD), that builds upon and complements previous work on domain generalization.
            We find that existing novelty detection methods designed for OOD benchmarks perform worse than simple confidence-based methods on our CSD benchmark.
            We propose Domain Interpolation Sensitivity (DIS), based on the simple hypothesis that interpolation between the test input and randomly sampled inputs from the training domain, offers sufficient information to distinguish between the training domain and unseen domains under covariate shift.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/phl_teaser.png" ><img class="figure-img" src="images/teasers/phl_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2110.08438" class="papertitle">Unsupervised Natural Language Inference Using PHL Triplet Generation</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <a class="author">Neeraj Varshney</a>,
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <br>
          <a href="https://arxiv.org/pdf/2110.08438.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/nrjvarshney/unsupervised_NLI" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://aclanthology.org/2022.findings-acl.159.mp4" class="badge badge-video badge-sm text-decoration-none">video</a>
          <p class="tag">
            Natural Language Inference (NLI) under three low-data settings (with missing labels; with missing labels and hypothesis; and with missing labels, hypotheses, and premises). A procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data.
            State-of-the-art results under all three "unsupervised" settings.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/sdro_teaser.png" ><img class="figure-img" src="images/teasers/sdro_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2110.07165" class="papertitle">Semantically Distributed Robust Optimization for Vision-and-Language Inference</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author">Abhishek Chaudhary</a>, 
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <br>
          <a href="https://arxiv.org/pdf/2110.07165.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/ASU-APG/VLI_SDRO" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://youtu.be/VsN23DYhuEg" class="badge badge-video badge-sm text-decoration-none mb-1">video</a>
          <p class="tag">
            SDRO: a distributed robust optimization method that operates with linguistic transformations of sentence inputs, SISP: a suit of semantics-inverting (SI) and semantics-preserving (SP) linguistic transformations, and an ensembling technique for vision-and-language inference.
          </p>
        </div>
      </div>



      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/genvsrob_teaser.png" ><img class="figure-img" src="images/teasers/genvsrob_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2203.07653" class="papertitle">Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</a>
          <br/><venue>ACL Findings 2022</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>, 
          <a class="author" href="https://luomancs.github.io/">Man Luo</a>, 
          <a class="author" href="https://scholar.google.com/citations?user=-7LK2SwAAAAJ">Swaroop Mishra</a>,
          <a class="author" href="https://www.linkedin.com/in/bhavdeep-singh-sachdeva-767a3b7a">Bhavdeep Singh Sachdeva</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2203.07653.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://youtu.be/nctzQo-TE9M" class="badge badge-video badge-sm text-decoration-none mb-1">video</a>
          <p class="tag">
            In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).
            This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/waldo_teaser.PNG" ><img class="figure-img" src="images/teasers/waldo_teaser.PNG"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2203.16682" class="papertitle">To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo</a>
          <br/><venue>ACL 2022</venue>
          <br>
          <a class="author" href="https://scholar.google.com/citations?user=n4IrxbUAAAAJ&hl=en">Yiran Luo</a>, 
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2203.16682.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/fpsluozi/tofindwaldo" class="badge badge-data badge-sm text-decoration-none mb-1">data</a>
          <p class="tag">
            We present a debiased dataset for the Person Centric Visual Grounding (PCVG) task. For instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image). The debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/bionr_teaser.png" ><img class="figure-img" src="images/teasers/bionr_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2201.07745" class="papertitle">Improving Biomedical Information Retrieval with Neural Retrievers</a>
          <br/><venue>AAAI 2022</venue>
          <br>
          <a class="author" href="https://pratyay-banerjee.github.io/">Man Luo</a>,
          <a class="author" href="https://scholar.google.com/citations?user=ItxA4esAAAAJ"> Arindam Mitra </a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2201.07745.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            We seek to improve information retrieval (IR) using neural retrievers (NR) in the biomedical domain, using a three-pronged approach. (1) a template-based question generation method, (2) two novel pre-training tasks that are closely aligned to the downstream task of information retrieval, (3) the ``Poly-DPR'' model which encodes each context into multiple context vectors. 
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/spatial_vqa_teaser.png" ><img class="figure-img" src="images/teasers/spatial_vqa_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2109.01934" class="papertitle">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering</a>
          <br/><venue>ICCV 2021</venue>
          <br>
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2109.01934.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            VQA models trained with two additional objectives: object centroid estimation and relative position estimation, lead to improved performance on spatial reasoning questions (in GQA) in fully supervised and few shot settings as well as improved O.O.D. generalization.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/weaqa_teaser.png"><img class="figure-img" src="images/teasers/weaqa_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2012.02356" class="papertitle">WeaQA: Weak Supervision via Captions for Visual Question Answering</a>
          <br/><venue>ACL 2021 Findings</venue>
          <br>
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.02356.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">
            We show that models can be trained without any human-annotated Q-A pairs, but only with images and associated text captions.  Our experiments suggest gains on benchmark with shifted priors (VQA-CP) over baselines which use full supervision from human-authored QA data.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/hallucinet_teaser.png"><img class="figure-img" src="images/teasers/hallucinet_teaser.png"></a>
        </div>
        <div class="col-md-8">
          <a href="https://arxiv.org/abs/2004.08614" class="papertitle">HalluciNet: Scene Completion by Exploiting Object Co-occurrence Relationships</a>
          <br/><venue class="arxiv">CVPR 2021 Workshop</venue>, "AI for Content Creation"
          <br>
          <a class="author" href="https://kuldeepkulkarni.github.io/">Kuldeep Kulkarni</a>, 
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://scholar.google.com/citations?user=1NzCrUgAAAAJ&hl=en">Rajhans Singh</a>,
          <a class="author" href="https://pavanturaga.com/">Pavan Turaga</a>,
          <a class="author" href="http://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a>
          <br>
          <a href="https://arxiv.org/pdf/2004.08614.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <p class="tag">             
            Scene completion from sparse and incomplete label maps.
            `Halluci-Net' is a 2-stage method that 
            captures the object co-occurrence relationships,
            to produce dense label maps from incomplete labelmaps and object boundaries,
            for image synthesis.
          </p>
        </div>
      </div>


      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/ttlrc_teaser.png"><img class="figure-img" src="images/teasers/ttlrc_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2103.11263" >
            Self-Supervised Test-Time Learning for Reading Comprehension
          </a>
          <br/><venue>NAACL 2021</venue>
          <br>
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>
          <br>
          <a href="https://arxiv.org/pdf/2103.11263.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>

          <p class="tag">
            Unsupervised Reading Comprehension method that operates directly on a single test passage.
            Synthetic QA pairs are generated from the passage, and models are trained on these.
            When a new human-authored test question appears, models infer answers better than previous unsupervised methods.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/agat_teaser.png"><img class="figure-img" src="images/teasers/agat_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a class="papertitle" href="https://arxiv.org/abs/2012.01806" >
            Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
          </a>
          <br/><venue>AAAI 2021</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://rushila.com/">Rushil Anirudh</a>,
          <a class="author" href="https://people.llnl.gov/kailkhura1">Bhavya Kailkhura</a>,
          <a class="author" href="https://jjthiagarajan.com/">Jayaraman J. Thiagarajan</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2012.01806.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/AGAT" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>

          <p class="tag">
            An adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to attributes-space.
            Studies robustness to semantic shifts that are beyond L-p norm perturbations, on 3 types of naturally occurring perturbations &mdash; object-related shifts, geometric transformations, and common image corruptions.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/mutant_teaser.png"><img class="figure-img" src="images/teasers/mutant_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="" class="papertitle">MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/abs/2009.08566" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/tejas-gokhale/vqa_mutant" class="badge badge-data badge-sm text-decoration-none mb-1">data</a>
          <p class="tag">
            MUTANT is a training paradigm that exposes VQA models to perceptually similar, yet semantically distinct <i>mutations</i> of the input image or question.
            We use a pairwise consistency loss between answers to original and mutant inputs as a regularization, along with an answer embedding NCE loss.
            MUTANTimproves generalization of VQA models under Changing Priors.
          </p>            
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/v2c_teaser.png"><img class="figure-img" src="images/teasers/v2c_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2003.05162" class="papertitle">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning
          </a>
          <br/><venue>EMNLP 2020</venue>
          <br>
          <a class="author" href="https://www.public.asu.edu/~zfang29/">Zhiyuan Fang*</a>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          <a href="https://arxiv.org/pdf/2003.05162.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
          <a href="https://github.com/jacobswan1/Video2Commonsense" class="badge badge-code badge-sm text-decoration-none mb-1">code</a>
          <a href="https://asu-active-perception-group.github.io/Video2Commonsense/" class="badge badge-web badge-sm text-decoration-none">web</a>
          <p class="tag">
            Actions in videos are inherently linked to latent social and commonsense aspects.
            We present the first work on generating commonsense captions directly from videos, to describe latent intentions, attributes, and effects of humans in videos.
            <!-- We present a new dataset (V2C) that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.  -->
            Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions.
          </p>      
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/vqalol_teaser.png"><img class="figure-img" src="images/teasers/vqalol_teaser.png"></a>
        </div>
        <div class="col-md-8 mb-3">
          <a href="https://arxiv.org/abs/2002.08325" class="papertitle">VQA-LOL: Visual Question Answering under the Lens of Logic
          </a>
          <br><venue>ECCV 2020</venue>
          <br>
          <b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://pratyay-banerjee.github.io/">Pratyay Banerjee</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
          <br>
          
          <a href="https://arxiv.org/pdf/2002.08325.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>,
          <a href="https://github.com/ASU-APG/vqa_lol" class="badge badge-code badge-sm text-decoration-none">code</a> 
          <a href="https://huggingface.co/datasets/tgokhale/vqa_lol" class="badge badge-data badge-sm text-decoration-none">data</a>
          <a href="./vqa_lol.html" class="badge badge-web badge-sm text-decoration-none">web</a>
          <a href="https://youtu.be/u6dEKvwla9M" class="badge badge-video badge-sm text-decoration-none">video</a>
          <p class="tag">
            VQA models struggle at negation, antonyms, conjunction, disjunction!
            We show a capability of answering logically composed questions with our novel modules and datasets, while retaining performance on VQA data.
          </p>
        </div>
      </div>

      <div class="row paper-block rounded justify-content-center bg-light">
        <div class="col-md-4-paper mb-3">
          <a href="images/teasers/blocksworld_teaser.PNG"><img class="figure-img" src="images/teasers/blocksworld_teaser.PNG"></a>
        </div>
        <div class="col-md-8 paper-block">
          <a href="https://arxiv.org/abs/1905.12042" class="papertitle">Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs
          </a>
          <br><venue>CVPR 2019 Workshop</venue>, Vision Meets Cognition
          <br><b><a class="author">Tejas Gokhale</a></b>,
          <a class="author" href="https://shailaja-sampat.mystrikingly.com/">Shailaja Sampat</a>,
          <a class="author" href="https://www.public.asu.edu/~zfang29">Zhiyuan Fang</a>,
          <a class="author" href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
          <a class="author" href="https://www.public.asu.edu/~cbaral">Chitta Baral</a>,
          <br>
          <a href="https://arxiv.org/pdf/1905.12042.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>, 
          <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">[CVPR-VMC Paper]</a>
          <a href="https://github.com/ASU-APG/BIRD_Code" class="badge badge-code badge-sm text-decoration-none">code</a>
          <a href="https://asu-apg.github.io/bird_dataset_web/" class="badge badge-web badge-sm text-decoration-none">web</a>
          <p class="tag">
            Given two images (source, target) with different object configurations, 
            what is the sequence of steps to re-arrange source to match target?
            For this reasoning task, our modular approach that contains a visual encoder and an event-sequencer/planner,
            and exhibits inductive generalization.
          </p>
        </div>
      </div>
  </div>
</div>

<hr/>

<div class="container" style="background-color: #fcfcfc;">
  <h3 id="funding"> SUPPORT / SPONSORS  </h3>
  <div class="row mb-3 justify-content-center">
    <div class="logo">
      <a href="https://www.darpa.mil/">
        <img class="logo_img" src="images/logos/darpa_logo.png"></a>
    </div>
    <div class="logo">
      <a href="https://research.umbc.edu/">
        <img class="logo_img" src="images/logos/umbc_transparent.png"></a>
    </div>
    <div class="logo">
      <a href="https://www.microsoft.com/en-us/research/">
        <img class="logo_img" src="images/logos/msr_logo.png">
      </a>
    </div>
    <div class="logo">
      <a href="https://hltcoe.jhu.edu/">
        <img class="logo_img" src="images/logos/hltcoe_logo.webp">
      </a>
    </div>
  </div>
</div>
<hr/>

<div class="container" style="background-color: #fcfcfc;">
  <h3 id="people"> Team </h3>
  <b> Ph.D. Students</b>
  <div class="row mb-3 justify-content-left">
    <div class="profile">
      <a href="https://sourajitcs.github.io/" class="name">
        <img class="profile_img" height="120px" src="images/people/sourajitsaha.jpg">
        <b>Sourajit Saha</a></b>
        <br /> Ph.D. CS, UMBC
    </div>
    <div class="profile">
      <a href="https://realziweizhang.github.io/" class="name">
        <img class="profile_img" height="120px" src="images/people/ziweizhang.png">
        <b>Ziwei Zhang</a></b>
        <br>Ph.D. CS, UMBC
    </div>
    <div class="profile">
      <a href="https://www.linkedin.com/in/shivanand-kundargi/" class="name">
        <img class="profile_img" height="120px" src="images/people/shivanandkundargi.jpeg">
        <b>Shivanand Kundargi</a></b>
        <br>Ph.D. CS, UMBC
    </div>
    <div class="profile">
      <a href="https://ztizzle.com/" class="name">
        <img class="profile_img" height="120px" src="images/people/jordanturley.jpeg">
        <b>Jordan Turley</a></b>
        <br>Ph.D. CS, UMBC
    </div>
    <div class="profile">
      <a href="https://www.linkedin.com/in/dylan-lang-64b654183/" class="name">
        <img class="profile_img" height="120px" src="images/people/dylanlang.jpg">
        <b>Dylan Lang</a></b>
        <br/> Ph.D. CS, UMBC
      </a>
    </div>

    <div class="profile">
      <img class="profile_img" height="120px" src="images/people/unclesam.webp">
      <b>This could be you!</b><br />
      <a href="https://forms.gle/X47T9QNcUnTpyb9A8">I'm hiring!</a>
      </p>
    </div>
  </div>
  <b> M.S. Students</b>
  <div class="row mb-3 justify-content-left">
    <div class="profile">
      <a href="#people" class="name">
        <img class="profile_img" height="120px" src="images/people/neelpatel.jpg">
        <b>Neel Patel</a></b>
        <br/> M.S. CS, UMBC
      </a>
    </div>
  </div>

  <b> Affiliated Ph.D. Students </b>
  <div class="row mb-3 justify-content-left">
    <div class="profile">
      <a href="https://shengcheng.github.io/" class="name">
        <img class="profile_img" height="120px" src="images/people/shengcheng.png">
        <b>Sheng Cheng</a></b>
        <br/> Ph.D. CS, ASU
      </a>
    </div>
    <div class="profile">
      <a href="https://agneetchatterjee.com/" class="name">
        <img class="profile_img" height="120px" src="images/people/agneetchatterjee.jpg">
        <b>Agneet Chatterjee</a></b>
        <br/> Ph.D. CS, ASU
      </a>
    </div>

    <div class="profile">
      <a href="https://maitreyapatel.com/" class="name">
        <img class="profile_img" height="120px" src="images/people/maitreyapatel.jpg">
        <b>Maitreya Patel</a></b>
        <br/> Ph.D. CS, ASU
      </a>
    </div>

    <!-- <div class="profile">
      <a href="#people" class="name">
        <img class="profile_img" height="120px" src="images/people/ethanwisdom.jpg">
        <b>Ethan Wisdom</a></b>
        <br/> Ph.D. CS, ASU
      </a>
    </div> -->
    
  </div>
  <details><summary><a href="images/gokhale_math_genealogy.png">Mathematics Genealogy </a></summary></details>
  <br/>
  <details>
    <summary>List of All Collaborators</summary>
    <ul>
      <i><font color="#900C3F">(ordered chronologically)</font></i>
      <li>Aswin Sankaranarayanan, CMU</li>
      <li>Kuldeep Kulkarni, CMU</li>
      <li>Yezhou Yang, ASU</li>
      <li>Chitta Baral, ASU</li>
      <li>Pavan Turaga, ASU</li>
      <li>Rajhans Singh, ASU </li>
      <li>Shailaja Sampat, ASU</li>
      <li>Zhiyuan Fang, ASU</li>
      <li>Pratyay Banerjee, ASU</li>
      <li>Rushil Anirudh, LLNL</li>
      <li>Jay Thiagarajan, LLNL</li>
      <li>Bhavya Kailkhura, LLNL</li>
      <li>Abhishek Chaudhary, ASU</li>
      <li>Neeraj Varshney, ASU</li>
      <li>Swaroop Mishra, ASU </li>
      <li>Man Luo, ASU</li>
      <li>Bhavdeep Singh Sachdeva, ASU </li>
      <li>Yiran Luo, ASU</li>
      <li>Arindam Mitra, Microsoft Research </li>
      <li>Josh Feinglass, ASU</li>
      <li>Maitreya Patel, ASU</li>
      <li>Hamid Palangi, Microsoft Research</li>
      <li>Besa Nushi, Microsoft Research</li>
      <li>Vibhav Vineet, Microsoft Research</li>
      <li>Eric Horvitz, Microsoft Research</li>
      <li>Ece Kamar, Microsoft Research</li>
      <li>Sheng Cheng, ASU </li>
      <li>Ethan Wisdom, ASU </li>
      <li>Chaowei Xiao, ASU</li>
      <li>Agneet Chatterjee, ASU</li>
      <li>Sourajit Saha, UMBC</li>
      <li>Gabriela Ben Melech Stan, Intel</li>
      <li>Estelle Aflalo, Intel</li>
      <li>Vasudev Lal, Intel</li>
      <li>Sayak Paul, Huggingface</li>
      <li>Dhruba Ghosh, University of Washington</li>
      <li>Ludwig Schmidt, University of Washington</li>
      <li>Hannaneh Hajishirzi, University of Washington</li>
      <li>Abhiram Kusumba, ASU</li>
      <li>Changhoon Kim, Amazon</li>
    </ul>
  </details>
  <br/>
  <details><summary><a href="./music.html">Music</a></summary></details>
</div>



<hr />

<p align='right'>
  <i>
    Website theme inspirations: 
    <a href="https://www.alanesuhr.com/">Alane Suhr</a>, 
    <a href="http://stevemacn.github.io/">Stephen MacNeil</a>, 
    <a href="https://jonbarron.info/">Jon Barron</a>
  </i>
</p>

</body>
</html>
