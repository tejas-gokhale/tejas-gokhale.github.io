<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="author" content="Tejas Gokhale">
  <meta property="og:image" content="images/logos/ppr.png" />
  <title>PPR Seminar UMBC</title>
  <link rel="stylesheet" type="text/css" href="css/seminar.min.css">
  <link rel="icon" type="image/png" href="images/logos/ppr.png">
  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="icon" href="./favicon.png" type="image/png" />

  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
    table, tr, td {
      border: none;
    }
    table.fixed {table-layout:fixed; width:90px;}/*Setting the table width is important!*/
    table.fixed td {overflow:hidden;}/*Hide text outside the cell.*/
    table.fixed td:nth-of-type(1) {width:250px;}/*Setting the width of column 1.*/
    table.fixed td:nth-of-type(2) {width:250px;}/*Setting the width of column 2.*/
    table.fixed td:nth-of-type(3) {width:250px;}/*Setting the width of column 3.*/
    table.fixed td:nth-of-type(4) {width:250px;}/*Setting the width of column 4.*/
    table.fixed td:nth-of-type(5) {width:250px;}/*Setting the width of column 5.*/
  </style>
</head>

<body>
  <div class="container" style="background-color: #fcfcfc;">
	<div class="row mb-3 justify-content-center">
		<div class="col-md-2" style="font-family: Lato, Helvetica, arial, sans-serif;">
		<img class="logo-img" src="images/logos/ppr.png">
	</div>
	<div class="col-md-9">
		<br>
		<h1 >PPR Seminar</h1>
		<h2 >Advances in Perception, Prediction, and Reasoning</h2>
		
		<h4 style="color:grey;">Hosted by <a href="https://www.tejasgokhale.com/">Tejas Gokhale</a> at UMBC</h4>
		<hr/><br/>
	</div>
	

    <div class="col-md-12 mb-3" style="font-size:16px;font-family: Lato, Helvetica, arial, sans-serif;">
    	<a id="schedule"><h3>Schedule</h3></a>



    	<hr/>
		<div class="row paper-block rounded justify-content-center">
			<div class="col-md-2 date-center" >
				<big><b>Nov 27, 2023</b></big>
				<br>4:00 -- 5:15 PM 
				<br>ENGR 231
				<br><br><a href="" class="badge badge-web badge-sm text-decoration-none mb-1">Webex Link</a>
				<!-- <br><a href="" class="badge badge-video badge-sm text-decoration-none mb-1">Recording</a> -->
			</div>
			<div class="col-md-2-person mb-3">
				<img class="person-img" src="images/people/eadomdessalene.jpg"></a>
			</div>
			<div class="col-md-7 mb-3">
				<br/>
				<big><b> <a href="https://scholar.google.com/citations?user=3kjVPtcAAAAJ&hl=en"> Eadom Dessalene</a> <br/> Ph.D. Candidate, University of Maryland College Park</b></big>
				<br/><br/>
				<p> <b>Learning Actions from Humans in Video</b><br/>
					The prevalent computer vision paradigm in the realm of action understanding is to directly transfer advances in object recognition toward action understanding. In this presentation I discuss the motivations for an alternative embodied approach centered around the modelling of actions rather than objects and survey recent work of ours along these lines, as well as promising possible future directions.
				</p>

				<details><summary><b>Bio</b></summary>
					<small>
						Eadom Dessalene is a Ph.D. Candidate at University of Maryland, College Park, advised by Yiannis Aloimonos and Cornelia Fermuller in the Perception and Robotics Group. Eadom received his bachelors degree in Computer Science from George Mason University. He has made several important contributions to research on video understanding and ego-centric vision through publications in CVPR, ICLR, T-PAMI, and ICRA, as well as winning first place in the 2020 EPIC Kitchens Action Anticipation Challenge.
					</small>
				</details>
			</div>
		</div>
		<hr/>
		<div class="row paper-block rounded justify-content-center">
			<div class="col-md-2 date-center" >
				<big><b>Nov 29, 2023</b></big>
				<br>4:00 -- 5:15 PM 
				<br>ENGR 231
				<br><br><a href="" class="badge badge-web badge-sm text-decoration-none mb-1">Webex Link</a>
			</div>
			<div class="col-md-2-person mb-3">
				<img class="person-img" src="images/people/kowshikthopalli.jpeg"></a>
			</div>
			<div class="col-md-7 mb-3">
				<br/>
				<big><b> <a href="https://kowshikthopalli.github.io/"> Kowshik Thopalli</a> <br/> Postdoctoral Researcher, Lawrence Livermore National Laboratory</b></big>
				<br/><br/>
				<p> <b>Making Machine Learning Models Safer: Data and Model Perspectives</b><br/>
					As machine learning systems are increasingly deployed in real-world settings like healthcare, finance, and scientific applications, ensuring their safety and reliability is crucial. However, many state-of-the-art ML models still suffer from issues like poor out-of-distribution generalization, sensitivity to input corruptions, requiring large amounts of data, and inadequate calibration - limiting their robustness and trustworthiness for critical real-world applications.  In this talk, I will first present a broad overview of different safety considerations for modern ML systems. I will then proceed to discuss our recent efforts in making ML models safer from two complementary perspectives - (i) manipulating data and (ii) enriching the model capabilities by developing novel training mechanisms.  I will discuss our work on designing new data augmentation techniques for object detection followed by demonstrating how, in the absence of data from desired target domains of interest, one could leverage pre-trained generative models for efficient synthetic data generation.  Next, I will present a new paradigm of training deep networks called model anchoring and show how one could achieve similar properties to an ensemble but through a single model. I will specifically discuss how model anchoring can significantly enrich the class of hypothesis functions being sampled and demonstrate its effectiveness through its improved performance on several safety benchmarks. I will conclude by highlighting exciting future research directions for producing robust ML models through leveraging multi-modal foundation models.
				</p>
				<details><summary><b>Bio</b></summary>
					<small>
						 Kowshik Thopalli is a Machine Learning Scientist and a post-doctoral researcher at Lawrence Livermore National Laboratory. His research focuses on developing reliable machine learning models that are robust under distribution shifts. He has published papers on a variety of techniques to address model robustness, including domain adaptation, domain generalization, and test-time adaptation using geometric and meta-learning approaches. His expertise also encompasses integrating diverse knowledge sources, such as domain expert guidance and generative models, to improve model data efficiency, accuracy, and resilience to distribution shifts.  He received his Ph.D. in 2023 from Arizona State University.
					</small>
				</details>
			</div>
		</div>
		<hr/>
		<div class="row paper-block rounded justify-content-center">
			<div class="col-md-2 date-center" >
				<big><b>Dec 04, 2023</b></big>
				<br>4:00 -- 5:15 PM 
				<br>ENGR 231
				<!-- <br><br><a href="" class="badge badge-web badge-sm text-decoration-none mb-1">Webex Link</a> -->
			</div>
			<div class="col-md-2-person mb-3" >
				<img class="person-img" src="images/people/manluo.jpeg"></a>
			</div>
			<div class="col-md-7 mb-3">
				<br/>
				<big><b> <a href="https://luomancs.github.io/"> Man Luo</a> <br/>Postdoctoral Research Fellow, Mayo Clinic</b></big>
				<br/><br/>
				<p><b>Advancing Multimodal Retrieval and Generation: From General to Biomedical Domains</b><br/>
					This talk explores advancements in multimodal retrieval and generation across general and biomedical domains. The first work introduces a multimodal retriever and reader pipeline for vision-based question answering, using image-text queries to retrieve and interpret relevant textual knowledge. The second work simplifies this approach with an efficient end-to-end retrieval model, removing dependencies on intermediate models like object detectors. The final part presents a biomedical-focused multimodal generation model, capable of classifying and explaining labels in images with text prompts. Together, these works demonstrate significant progress in integrating visual and textual data processing in diverse applications.
				</p>
				<details><summary><b>Bio</b></summary>
					<small>
						Dr Man Luo is a Postdoctoral Research Fellow at Mayo Clinic with Dr. Imon Banerjee and Dr. Bhavik Patel.
						Her research is at the intersection of information retrieval and reading comprehension within natural language processing (NLP) and multimodal domains, to retrieve and utilize external knowledge with efficiency and generalization. 
						Currently she is interested in knowledge retrieval, multimodal understanding, and applications of LLMs and VLMs in biomedical/healthcare application.  She earned her Ph.D. in 2023 from Arizona State University advised by Dr. Chitta Baral, and has collaborated at industrial research labs at Salesforce, Meta, and Google.
					</small>
				</details>
			</div>
		</div>
		<hr/>




  	</div>
  </div>
</body>


</body>
</html>
